{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # load and analysis\n",
    "import numpy as np # load and analysis\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "\n",
    "# model library\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,Callback, EarlyStopping\n",
    "from swa.keras import SWA # swa optimizer - https://pypi.org/project/keras-swa/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "TRAIN_PATH = './data/train.csv'\n",
    "TEST_PATH = './data/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler 구현\n",
    "# lr을 cyclic하게 변환해줌\n",
    "import math\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose &gt; 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gelu activation function -  Gaussian Error Linear Units (GELUs)\n",
    "# https://arxiv.org/abs/1606.08415\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "class Gelu(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Gelu, self).__init__(activation, **kwargs)\n",
    "        self.__name__='gelu'\n",
    "        \n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "get_custom_objects().update({'gelu': Gelu(gelu)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리(Data Cleansing & Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별도의 전처리 없음\n",
    "y_columns = ['layer_1', 'layer_2', 'layer_3', 'layer_4']\n",
    "X_train = df_train.drop(columns=y_columns)\n",
    "y_train = df_train[y_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 탐색적 자료분석(Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러가지 모델을 설계해 보면서 탐구 해보았지만 최종 결과에는 큰지장이 없어 제거함\n",
    "\n",
    "NeuralNet 모델을 사용하는 것이 맞다는 결론에 도달해 모든 피쳐를 사용하는 모델을 사용하기로함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 변수 선택 및 모델 구축(Feature Engineering & Initial Modeling)\n",
    "\n",
    "최종모델 선택에 도달하기까지의 모델 구현 과정의 아이디어를 기입하였습니다.\n",
    "\n",
    "- 1 Feature 추가\n",
    "\n",
    "모든 layer의 두께의 합을 예측 피쳐로 추가함 -> 결과에 큰 차이가 없어 최종 모델에서 제거\n",
    "\n",
    "- 2 Dropout 조절\n",
    "\n",
    "Underfit 되는 현상을 보고 0.5 에서 0.01까지 계속 낮추며 실험을함, 낮출수록 성능 증가\n",
    "실험을 하다보니 training set에 overfit되지 않는 모습을 보게되어 매우 작은 값으로 선택 (validset과 leaderboard상의 점수가 훨씬 높게 나옴)\n",
    "\n",
    "- 3 Underfit  \n",
    "\n",
    "Layer를 깊고 크게 쌓다보니 학습이 잘 되지않아 SWA, CosineAnnealing Scheduler, Gelu 등을 사용하여 최대한 학습하려고 함\n",
    "특히 Batchnorm 추가 뒤에 성능이 크게 증가\n",
    "\n",
    "- 4 K-Fold Ensemble\n",
    "\n",
    "구조가 다른 여러 모델을 사용해보았지만 크게 성능향상이 되지 않아 단일 모델의 K-Fold ensemble을 하였음\n",
    "\n",
    "- 5 기타 실험 모델들\n",
    "\n",
    "Batchnorm 추가 이후 0.8 -> 0.4 아래로 큰 향상\n",
    "CNN base - 0.44\n",
    "Layer-sum feature 추가 모델 - 0.38\n",
    "기타 Boosting Tree model - 1점대"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증(Model Tuning & Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "lr_d = 0.0\n",
    "patience = 200\n",
    "dr_rate = 0.01\n",
    "kfold = 13\n",
    "kf = KFold(n_splits=kfold, shuffle=True, random_state=7777)\n",
    "# train model\n",
    "for enum, (train_index,valid_index) in enumerate(kf.split(X_train,y_train)):\n",
    "    file_path = f\"bn_swa_nn_best_model_fold_{enum}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    cosine_scheduler = CosineAnnealingScheduler(T_max=100, eta_max=6e-4, eta_min=3e-5)\n",
    "    swa = SWA(start_epoch=20, lr_schedule='manual', swa_lr=3e-4, swa_freq=5, verbose=1,batch_size=4096)\n",
    "\n",
    "    kf_x_train = X_train.iloc[train_index]\n",
    "    xf_y_train = y_train.iloc[train_index]\n",
    "    \n",
    "    kf_x_val = X_train.iloc[valid_index]\n",
    "    kf_y_val = y_train.iloc[valid_index]\n",
    "\n",
    "    inp = Input(shape = (226,))\n",
    "    x = Dense(2017, activation=None)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(2013, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(1027, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(1023, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(517, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(509, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(503, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(307, activation='gelu')(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x_1 = Dense(1, activation=\"relu\", name='layer_1')(x)\n",
    "    x_2 = Dense(1, activation=\"relu\", name='layer_2')(x)\n",
    "    x_3 = Dense(1, activation=\"relu\", name='layer_3')(x)\n",
    "    x_4 = Dense(1, activation=\"relu\", name='layer_4')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[x_1,x_2,x_3,x_4])\n",
    "    model.summary()\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=lr, decay=lr_d))\n",
    "    model.fit(kf_x_train, \n",
    "             [np.array(xf_y_train)[:,0],np.array(xf_y_train)[:,1],np.array(xf_y_train)[:,2],np.array(xf_y_train)[:,3]],\n",
    "              batch_size=4096, epochs=10000,\n",
    "              validation_data = [kf_x_val, [np.array(kf_y_val)[:,0],np.array(kf_y_val)[:,1],np.array(kf_y_val)[:,2],\n",
    "                                            np.array(kf_y_val)[:,3]]],\n",
    "              verbose=2, callbacks=[early_stop,check_point,cosine_scheduler,swa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.iloc[:,1:]\n",
    "\n",
    "pred_test = np.zeros((len(X_test),4))\n",
    "\n",
    "for i in range(kfold):\n",
    "    print(i)\n",
    "    model = load_model(\"./bn_swa_nn_best_model_fold_{}.hdf5\".format(i))\n",
    "    pred = np.array(model.predict(X_test))\n",
    "    pred_test += np.transpose(pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pred_test /kfold\n",
    "sample_sub = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "\n",
    "submission = sample_sub+pred_test\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
