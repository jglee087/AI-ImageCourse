

Kaggle.com

- CPU연산 - 머신러닝 중에 random forest, XGboost, 강화학습

- GPU연산 - deep learning 계산에 유리, 병렬연산

GPU memory가 충분히 커야 한다.



### keras03_test.py

##### 입력 데이터의 형태

Scalar, Vector, Matrix, Tensor 

Scalar는 특정 좌표계와 의존하지 않는 물리량이다. 방향을 가지고 있지 않고 크기만 가지고 있는 물리량으로 표현된다. Vector는 방향과 크기로 결정되는 양으로 정의한다. Matrix는 1개 이상의 수나 식을 사각형의 배열로 나열한 것으로 가로줄을 행(row), 세로줄을 열(column)이라고 한다. Tensor는 좌표변환하에서 특정한 변환법칙(transformation law)을 따르는 물리량이다. 보통 행렬로 표현하는데, 일반적으로 n차원(dimension)의 m차(rank) 텐서는 $n^m$개의 원소를 가진다.

파이썬에서는 다음과 같이 표현할 수 있다.

```python
scalar= 3
vector=np.array([1,2,3,4,5]) 
matrix=np.array([ [1,2,3,4], [5,6,7,8]])
tensor=np.array([[ [1],[2],[3]], [[4],[5],[6]], [[7],[8],[9]] ])
```



```python
x=np.array([1,2,3,4,5,6,7,8,9,10])
x.shape
(10,)
xx=np.array([[1,2,3,4,5,6,7,8,9,10]])
xx.shape
(1,10)
```

<u>**x의 크기는(10,)이고 벡터라고 한다.**</u> xx는 (1,10) 모양을 가진 행렬(matrix)이다.



### keras04_val.py

#### Validation set

훈련하는 중간에 검증을 하면 성능이 좋아질 것이다. 데이터는 3가지로 나눈다. 즉, train set, test set, validation set으로 나눈다. 

model은 train, validation, test, predict 계산 순서대로 진행한다.

![img](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)

validation set은 machine learning 또는 통계에서 기본적인 개념 중 하나이다. 하지만 실무를 할때 귀찮은 부분 중 하나이며 간과되기도 한다. 그냥 training set으로 training을 하고 test만 하면 되지 왜 귀찮게 validation set을 나누는 것일까요?

**validation set을 사용하는 이유**는 간단하다. 모델의 성능을 평가하기 위해서이다.  training을 한 후에 만들어진 모형이 잘 예측을 하는지 그 성능을 평가하기 위해서 사용한다. training set의 일부를 모델의 성능을 평가하기 위해서 희생하는 것입니다. 하지만 이 희생을 감수하지 못할만큼 **data set의 크기가 작다면 cross-validation**이라는 방법을 쓰기도 한다. **cross-validation**은 training set을 **k-fold 방식**을 통해 쪼개서 모든 데이터를 training과 validation 과정에 사용할 수 있다. 



### keras05_split.py, keras06_split2.py

#### Train_test_split

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2,train_size=0.8,shuffle=False)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,
                                                  test_size = 0.25, train_size =0.75, shuffle=False)

```



**random_state**는 난수 초기값을 지정할 수 있는 옵션이다.

즉, random state는 난수를 정해진 순서로 같은 발생키는 역할을 한다. 예를 들면, 같은 입력 데이터로 계산을 반복했을때 같은 결과 구현하는데 사용한다. random_state를 고정하면 똑같은 작업을 재현 가능하다. random_state는 int, randomstate instance 또는 None 값을 사용한다. 정수 값을 입력하면 숫자를 random하게 생성할때 사용되는 seed 숫자로 사용되며, None을 입력하면 np.random에서 제공하는 random number generator가 사용됩니다.



### 데이터의 종류

기본 데이터를 나누는 방법

|            | X                   | Y                                                            |
| ---------- | ------------------- | ------------------------------------------------------------ |
| train      | x_train(known)      | y_train(known)                                               |
| test       | x_test(known)       | y_test(known)                                                |
| validation | x_validation(known) | y_validation(known)                                          |
| predict    | x_predict           | **predict(x_predict)** - 예측하고자 하는 값 <br />즉, 알고싶은 값 |



### keras08_R2.py

#### $R^2$ score(R-squared)

Regression model의 성능을 측정하기 위해, mean value로 예측하는 단순 모델에 비해 상대적으로 얼마나 성능이 나오는지를 측정한다.
$$
R^{2}=1-\frac{SSE}{SST}
$$
SST: Sum of Square Total: 편차의 제곱합

SSE: Sum of Square Error: 오차의 제곱합



![img](https://t1.daumcdn.net/cfile/tistory/253105495922766235)

이때 그냥 mean value를 예측하는 regression model을 쓴다면, 우측항이 1이 되므로 $R^2 = 0$이 된다. 또한 $R^2$값이 1이면, 오차^2 = 0인 상황이므로 **training error가 0**인 것을 의미한다.

만약 $R^2$가 0보다 작은 음수를 내는 경우는,  **편차^2**보다 **오차^2**이 더 큰 상황이다. 즉, 이는 regression model이 그냥 mean value로 예측하는 모델보다 예측성능이 더 낮은 경우를 의미한다.



회귀모델을 평가하는 지표는 크게 1. MSE, MAE, RMSE, RMAE 와 2. $R^2$가 있다. 

결론은 RMSE 값은 최대한 작고 $R^2$ score는 1에 가까울수록 좋은 모델이라 할 수 있다.



### INPUT DATA DIMENSION 

<u>**<<행 무시? 열 우선>>**</u>

10개 짜리 스칼라의 묶음을 벡터라고 하고 이는 1행 10열의 행렬로 볼 수 있다. 행이 없다. 그 다음부터는 shape 구조로 간다. 노드의 개수가 열에 들어간다. 

처음 은닉층에 집어넣을 수 있는 옵션인 **input_dim, input_shape**이 있는데 매우 중요한 옵션이다. 

```python
x=np.array(range(1,11))
print(x.shape)
(10,)
print(x) # this is vector not matrix
1,2,3,4,5,6,7,8,9,10
```



```python
#1. 데이터
x=np.array([range(1,11),range(11,21)])
y=np.array([range(1,11),range(11,21)])
x.shape
(2,10)
y.shape
(2,10)
```



행은 추가 될수도 있고 삭제될수도 있다. 하지만 열은 그 자체로 중요한 의미가 있다. 

What means **input_dim=2**가 의미하는 것은 입력할 때 들어가는 열의 개수로 2개의 열을 입력한다는 뜻이다.

- input_dim=2, input_shape (2,) 는 같은 의미로 벡터가 2개 or 열이 2개

- input_dim=1, input_shape (1,) 는 같은 의미로 1개 or 열이 1개



#### 행렬의 크기

```python
[1,2,3] ### (1,3)
[[1,2,3],[1,2,3]] ### (2,3)
[[[1,2,3],[1,2,3]]] ### (1,2,3)
[[[1,2],[3,4]],[[1,2],[3,4]]] ### (2,2,2)
[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]] ### (3,5)
```



```python
x1=np.array(range(1,101)) # (100,)
x1.shape
(100,)
x1=np.transpose(x1)
x1.shape
(100,)
x1=x1.reshape(1,100)
x1.shape
(1,100)

x2=np.array([range(1,101)]) # (1,100)
x2.shape
(1,100)
```

**(100,)의 모양을 바꾸려면 transpose 대신에 reshape를 사용해야 한다.** <u>**INPUT DIM !! 벡터는 반드시 신경써줘야 한다.**</u>



#### 좋은 데이터를 가지고 나쁜 결과를 만들 수 있을까?

조정할 수 있는 하이퍼 파라미터의 종류는 레이어의 깊이 , 노드의 개수, 배치 사이즈, Epoch, metrics 이다.

층의 깊이가 깊고 노드의 개수가 오히려 많으면 과적합이 발생해서 정확도가 낮아지게 된다.
