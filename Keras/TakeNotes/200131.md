## Review(20-01-31)

LSTM 여러 층 쌓기 **return_sequences, Reshape**

input_shape 2차원(열의 개수, feature) -> output 1차원, LSTM의 아웃풋 3차원(?)



**1개의 LSTM 층만 주어졌을 때**

```python
# 입력 데이터 형태: (None, 3, 1)
model.add(LSTM(10, input_shape=(3,1))
# LSTM을 지나 output은 (None,10)
model.add(Desne(5))
# Dense 층을 지나 output은 (None, 5)
model.summary()
```

**3개의 LSTM 층이 연속으로 주어졌을 때**

```python
# 입력 데이터 형태: (None, 3, 1)
model.add(LSTM(10, input_shape=(3,1), return_sequneces=True)
# LSTM1을 지나 output은 (None,3,10)
model.add(LSTM(5), return_sequneces=True)
# LSTM2을 지나 output은 (None,3,5)
model.add(LSTM(2), return_sequneces=False)
# LSTM3을 지나 output은 (None,3,2)        
model.add(Desne(5))
# Dense 층을 지나 output은 (None, 5)
model.summary()
```



### **Data Preprocessing**

**1. Scailing**

1. MinMaxScaler() (정규화, Normalization)
2. StandardScaler() (표준화, Standardization)

StandardScaler는 기본적으로 열(column)을 기준으로 계산한다.



Dropout(숫자=노드를 뺄 만큼의 비율)

BatchNormalization( ) 

-------------



### keras20_tensorboard.py

옵션은 log_dir: 기록 저장소 (.은 현재 위치), histogram_freq=0, write_graph: 그래프 작성, write_images: 이미지 작성

- 명령 프롬프트에서 graph 이전 폴더까지 가서 tensorboard --logdir=./graph를 실행

- localhost:6006 로 연결



#### keras21_model_save, model_load.py

저장된 모델을 불러와서 model.add를 할때에는 이름을 자동으로 할당하니깐 **겹치지 않게 이름을 따로 지정**해줘야 한다. 이미 저장된 모델에 이어서 붙이는 것을 customizing이라 한다.

에러 발생, stackoverflow, 구글에서 찾아보고 한글 번역

?? 함수형 모델로 이어가는 방법은?



#### keras22_univariate.py

```python
def split_sequences(sequence, n_steps):
    
# sequence는 입력받은 데이터 셋
# n_steps는 자를 개수
# 시계열 데이터 구성!!
```



알파고~ 바둑. 경우의 수는 361! 

AlphaGoLee 버전(기보학습). AlphaGoMaster 버전(강화학습)

- Machine learning - 차원 축소, Feature Importances

* 강화학습 - CPU가 중요.



#### keras23_multiple1.py

DNN 방법 사용



이미지 분석도 LSTM도 가능하다!!!! 

x의 형태는 **(3,2), 3행 2열**이다. 위에 py에서는 x의 형태는 (3,)이다.

reshape할 때는 원소의 개수가 항상 같아야 한다.

**데이터 shape**



#### keras23_multiple2.py

LSTM 방법 사용

**데이터 shape**



#### keras24_m_parallel.py, keras24_m_parallel2.py, keras24_m_parallel3.py



### <u>transpose와 reshape의 차이는????</u>