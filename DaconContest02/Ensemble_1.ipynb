{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "h=datetime.today().month      # 현재 월 가져오기\n",
    "m=datetime.today().day        # 현재 일 가져오기\n",
    "\n",
    "hm='0'+str(h)+str(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Dropout, Activation, BatchNormalization, Concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers, callbacks\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from swa.keras import SWA # swa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train = pd.read_csv('/home/lab21/data/train.csv', index_col=0)\n",
    "train_2 = pd.read_csv('/home/lab21/data/train_x_0.2_99.8.csv', index_col=0)\n",
    "\n",
    "test = pd.read_csv('/home/lab21/data/test.csv', index_col=0)\n",
    "sample_submission = pd.read_csv('/home/lab21/data/sample_submission.csv', index_col=0)\n",
    "\n",
    "# Train 데이터의 타입을 Sample_submission에 대응하는 가변수 형태로 변환\n",
    "column_number = {}\n",
    "for i, column in enumerate(sample_submission.columns):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "train['type_num'] = train['type'].apply(lambda x : to_number(x, column_number))\n",
    "\n",
    "# 모델에 적용할 데이터 셋 준비 \n",
    "#x = train.drop(columns=['type', 'type_num'], axis=1)\n",
    "y = train['type_num']\n",
    "\n",
    "x = train_2.drop(columns=['fiberID'], axis=1)\n",
    "\n",
    "test_x = test.drop(columns=['fiberID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "test_x=np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.astype('float32')\n",
    "test_x=test_x.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# scaler1=RobustScaler()\n",
    "scaler2=StandardScaler()\n",
    "\n",
    "scaler2.fit(x)\n",
    "x_train=scaler2.transform(x)\n",
    "test_x =scaler2.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.callbacks import Callback\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GELU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "\n",
    "class Gelu(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Gelu, self).__init__(activation, **kwargs)\n",
    "        self.__name__='gelu'\n",
    "        \n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "get_custom_objects().update({'gelu': Gelu(gelu)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0008\n",
    "lr_d = 0\n",
    "patience =  300\n",
    "drop = 0.2\n",
    "nepoch = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = 7\n",
    "skf = StratifiedKFold(n_splits=skfold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 10:24:15.443186\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start=datetime.datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model\n",
    "- EarlyStop, CosineScheduler, CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lab21/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/lab21/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 171413 samples, validate on 28578 samples\n",
      "Model uses batch normalization. SWA will require last epoch to be a forward pass and will run with no learning rate\n",
      "Epoch 1/800\n",
      " - 41s - loss: 0.6990 - acc: 0.7800 - val_loss: 1.5716 - val_acc: 0.6123\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.57157, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 2/800\n",
      " - 37s - loss: 0.5305 - acc: 0.8203 - val_loss: 0.9898 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.57157 to 0.98981, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 3/800\n",
      " - 38s - loss: 0.5024 - acc: 0.8276 - val_loss: 0.8684 - val_acc: 0.6840\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.98981 to 0.86841, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 4/800\n",
      " - 37s - loss: 0.4842 - acc: 0.8334 - val_loss: 1.2238 - val_acc: 0.5734\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.86841\n",
      "Epoch 5/800\n",
      " - 37s - loss: 0.4721 - acc: 0.8367 - val_loss: 0.6616 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86841 to 0.66160, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 6/800\n",
      " - 37s - loss: 0.4640 - acc: 0.8388 - val_loss: 1.2511 - val_acc: 0.6632\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.66160\n",
      "Epoch 7/800\n",
      " - 37s - loss: 0.4572 - acc: 0.8413 - val_loss: 2.0747 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66160\n",
      "Epoch 8/800\n",
      " - 37s - loss: 0.4509 - acc: 0.8430 - val_loss: 1.2473 - val_acc: 0.6570\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66160\n",
      "Epoch 9/800\n",
      " - 37s - loss: 0.4456 - acc: 0.8449 - val_loss: 1.1895 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66160\n",
      "Epoch 10/800\n",
      " - 38s - loss: 0.4407 - acc: 0.8468 - val_loss: 1.8082 - val_acc: 0.5471\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66160\n",
      "Epoch 11/800\n",
      " - 37s - loss: 0.4376 - acc: 0.8472 - val_loss: 1.4899 - val_acc: 0.6294\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66160\n",
      "Epoch 12/800\n",
      " - 37s - loss: 0.4324 - acc: 0.8488 - val_loss: 1.9821 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.66160\n",
      "Epoch 13/800\n",
      " - 37s - loss: 0.4291 - acc: 0.8500 - val_loss: 1.7379 - val_acc: 0.6055\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66160\n",
      "Epoch 14/800\n",
      " - 37s - loss: 0.4265 - acc: 0.8504 - val_loss: 2.7269 - val_acc: 0.4659\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66160\n",
      "Epoch 15/800\n",
      " - 38s - loss: 0.4230 - acc: 0.8513 - val_loss: 1.2501 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66160\n",
      "Epoch 16/800\n",
      " - 37s - loss: 0.4207 - acc: 0.8521 - val_loss: 1.0590 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66160\n",
      "Epoch 17/800\n",
      " - 37s - loss: 0.4197 - acc: 0.8520 - val_loss: 1.2905 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66160\n",
      "Epoch 18/800\n",
      " - 37s - loss: 0.4154 - acc: 0.8547 - val_loss: 1.1227 - val_acc: 0.6170\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.66160\n",
      "Epoch 19/800\n",
      " - 37s - loss: 0.4126 - acc: 0.8544 - val_loss: 0.8856 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.66160\n",
      "Epoch 20/800\n",
      " - 38s - loss: 0.4114 - acc: 0.8546 - val_loss: 1.1063 - val_acc: 0.6313\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.66160\n",
      "Epoch 21/800\n",
      " - 37s - loss: 0.4096 - acc: 0.8555 - val_loss: 1.0087 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.66160\n",
      "Epoch 22/800\n",
      " - 37s - loss: 0.4071 - acc: 0.8568 - val_loss: 1.3675 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.66160\n",
      "Epoch 23/800\n",
      " - 37s - loss: 0.4039 - acc: 0.8578 - val_loss: 1.3501 - val_acc: 0.6601\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.66160\n",
      "Epoch 24/800\n",
      " - 37s - loss: 0.4048 - acc: 0.8576 - val_loss: 1.0370 - val_acc: 0.6161\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.66160\n",
      "Epoch 25/800\n",
      " - 38s - loss: 0.4007 - acc: 0.8589 - val_loss: 0.8071 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.66160\n",
      "Epoch 26/800\n",
      " - 38s - loss: 0.3991 - acc: 0.8585 - val_loss: 1.0291 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.66160\n",
      "Epoch 27/800\n",
      " - 37s - loss: 0.3980 - acc: 0.8594 - val_loss: 1.1971 - val_acc: 0.6630\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66160\n",
      "Epoch 28/800\n",
      " - 37s - loss: 0.3964 - acc: 0.8598 - val_loss: 0.7270 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66160\n",
      "Epoch 29/800\n",
      " - 38s - loss: 0.3952 - acc: 0.8602 - val_loss: 0.6428 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.66160 to 0.64279, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 30/800\n",
      " - 38s - loss: 0.3935 - acc: 0.8604 - val_loss: 0.6222 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.64279 to 0.62223, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 31/800\n",
      " - 37s - loss: 0.3917 - acc: 0.8618 - val_loss: 1.2633 - val_acc: 0.6468\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.62223\n",
      "Epoch 32/800\n",
      " - 37s - loss: 0.3904 - acc: 0.8613 - val_loss: 0.9289 - val_acc: 0.6612\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.62223\n",
      "Epoch 33/800\n",
      " - 39s - loss: 0.3879 - acc: 0.8627 - val_loss: 0.9003 - val_acc: 0.7048\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.62223\n",
      "Epoch 34/800\n",
      " - 40s - loss: 0.3891 - acc: 0.8619 - val_loss: 0.5821 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.62223 to 0.58207, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 35/800\n",
      " - 40s - loss: 0.3856 - acc: 0.8641 - val_loss: 0.6424 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.58207\n",
      "Epoch 36/800\n",
      " - 40s - loss: 0.3853 - acc: 0.8634 - val_loss: 1.0553 - val_acc: 0.6549\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.58207\n",
      "Epoch 37/800\n",
      " - 39s - loss: 0.3838 - acc: 0.8647 - val_loss: 0.8112 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.58207\n",
      "Epoch 38/800\n",
      " - 40s - loss: 0.3822 - acc: 0.8647 - val_loss: 0.4411 - val_acc: 0.8422\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.58207 to 0.44115, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 39/800\n",
      " - 40s - loss: 0.3808 - acc: 0.8644 - val_loss: 0.3826 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.44115 to 0.38264, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 40/800\n",
      " - 39s - loss: 0.3811 - acc: 0.8647 - val_loss: 0.4356 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.38264\n",
      "Epoch 41/800\n",
      " - 39s - loss: 0.3811 - acc: 0.8652 - val_loss: 0.4190 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.38264\n",
      "Epoch 42/800\n",
      " - 40s - loss: 0.3790 - acc: 0.8652 - val_loss: 0.4026 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.38264\n",
      "Epoch 43/800\n",
      " - 39s - loss: 0.3776 - acc: 0.8654 - val_loss: 0.3934 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.38264\n",
      "Epoch 44/800\n",
      " - 39s - loss: 0.3776 - acc: 0.8659 - val_loss: 0.3786 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.38264 to 0.37858, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 45/800\n",
      " - 39s - loss: 0.3772 - acc: 0.8664 - val_loss: 0.3976 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.37858\n",
      "Epoch 46/800\n",
      " - 40s - loss: 0.3775 - acc: 0.8661 - val_loss: 0.3976 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.37858\n",
      "Epoch 47/800\n",
      " - 42s - loss: 0.3768 - acc: 0.8661 - val_loss: 0.3931 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.37858\n",
      "Epoch 48/800\n",
      " - 42s - loss: 0.3764 - acc: 0.8654 - val_loss: 0.3788 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.37858\n",
      "Epoch 49/800\n",
      " - 42s - loss: 0.3757 - acc: 0.8667 - val_loss: 0.4029 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.37858\n",
      "Epoch 50/800\n",
      "\n",
      "Epoch 00050: starting stochastic weight averaging\n",
      " - 42s - loss: 0.3764 - acc: 0.8666 - val_loss: 0.3894 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.37858\n",
      "Epoch 51/800\n",
      " - 42s - loss: 0.3760 - acc: 0.8670 - val_loss: 0.3753 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.37858 to 0.37528, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 52/800\n",
      " - 41s - loss: 0.3753 - acc: 0.8663 - val_loss: 0.3816 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.37528\n",
      "Epoch 53/800\n",
      " - 41s - loss: 0.3754 - acc: 0.8666 - val_loss: 0.3659 - val_acc: 0.8714\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.37528 to 0.36593, saving model to /home/lab21/20200220/model/best_ensembl2_0.hdf5\n",
      "Epoch 54/800\n",
      " - 42s - loss: 0.3743 - acc: 0.8668 - val_loss: 0.3919 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.36593\n",
      "Epoch 55/800\n",
      " - 42s - loss: 0.3747 - acc: 0.8669 - val_loss: 0.3795 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.36593\n",
      "Epoch 56/800\n",
      " - 41s - loss: 0.3756 - acc: 0.8663 - val_loss: 0.3839 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.36593\n",
      "Epoch 57/800\n",
      " - 42s - loss: 0.3761 - acc: 0.8671 - val_loss: 0.5155 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.36593\n",
      "Epoch 58/800\n",
      " - 41s - loss: 0.3755 - acc: 0.8662 - val_loss: 0.3957 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.36593\n",
      "Epoch 59/800\n",
      " - 42s - loss: 0.3764 - acc: 0.8663 - val_loss: 0.3966 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.36593\n",
      "Epoch 60/800\n",
      " - 42s - loss: 0.3758 - acc: 0.8662 - val_loss: 0.3919 - val_acc: 0.8634\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.36593\n",
      "Epoch 61/800\n",
      " - 41s - loss: 0.3767 - acc: 0.8664 - val_loss: 0.4196 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.36593\n",
      "Epoch 62/800\n",
      " - 42s - loss: 0.3763 - acc: 0.8669 - val_loss: 0.6053 - val_acc: 0.7752\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.36593\n",
      "Epoch 63/800\n",
      " - 42s - loss: 0.3777 - acc: 0.8661 - val_loss: 0.5289 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.36593\n",
      "Epoch 64/800\n",
      " - 41s - loss: 0.3773 - acc: 0.8656 - val_loss: 0.4484 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.36593\n",
      "Epoch 65/800\n",
      " - 41s - loss: 0.3774 - acc: 0.8657 - val_loss: 0.6081 - val_acc: 0.7631\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.36593\n",
      "Epoch 66/800\n",
      " - 41s - loss: 0.3800 - acc: 0.8655 - val_loss: 0.7289 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.36593\n",
      "Epoch 67/800\n",
      " - 41s - loss: 0.3799 - acc: 0.8653 - val_loss: 0.4901 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.36593\n",
      "Epoch 68/800\n",
      " - 41s - loss: 0.3798 - acc: 0.8654 - val_loss: 0.6461 - val_acc: 0.7592\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.36593\n",
      "Epoch 69/800\n",
      " - 41s - loss: 0.3784 - acc: 0.8656 - val_loss: 0.7024 - val_acc: 0.7872\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.36593\n",
      "Epoch 70/800\n",
      " - 42s - loss: 0.3810 - acc: 0.8647 - val_loss: 0.8495 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.36593\n",
      "Epoch 71/800\n",
      " - 42s - loss: 0.3812 - acc: 0.8640 - val_loss: 0.5153 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.36593\n",
      "Epoch 72/800\n",
      " - 42s - loss: 0.3818 - acc: 0.8639 - val_loss: 0.6214 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.36593\n",
      "Epoch 73/800\n",
      " - 41s - loss: 0.3828 - acc: 0.8638 - val_loss: 0.5348 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.36593\n",
      "Epoch 74/800\n",
      " - 42s - loss: 0.3829 - acc: 0.8636 - val_loss: 0.7103 - val_acc: 0.7701\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.36593\n",
      "Epoch 75/800\n",
      " - 42s - loss: 0.3823 - acc: 0.8644 - val_loss: 0.7285 - val_acc: 0.7398\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.36593\n",
      "Epoch 76/800\n",
      " - 42s - loss: 0.3831 - acc: 0.8635 - val_loss: 0.9506 - val_acc: 0.7079\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.36593\n",
      "Epoch 77/800\n",
      " - 41s - loss: 0.3838 - acc: 0.8639 - val_loss: 1.3077 - val_acc: 0.6131\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.36593\n",
      "Epoch 78/800\n",
      " - 41s - loss: 0.3856 - acc: 0.8637 - val_loss: 1.0116 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.36593\n",
      "Epoch 79/800\n",
      " - 41s - loss: 0.3851 - acc: 0.8626 - val_loss: 0.8215 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.36593\n",
      "Epoch 80/800\n",
      " - 42s - loss: 0.3844 - acc: 0.8631 - val_loss: 1.4196 - val_acc: 0.6106\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.36593\n",
      "Epoch 81/800\n",
      " - 42s - loss: 0.3849 - acc: 0.8632 - val_loss: 1.7032 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.36593\n",
      "Epoch 82/800\n",
      " - 41s - loss: 0.3853 - acc: 0.8623 - val_loss: 1.0220 - val_acc: 0.6393\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.36593\n",
      "Epoch 83/800\n",
      " - 41s - loss: 0.3870 - acc: 0.8621 - val_loss: 0.9324 - val_acc: 0.7053\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.36593\n",
      "Epoch 84/800\n",
      " - 42s - loss: 0.3852 - acc: 0.8631 - val_loss: 1.4912 - val_acc: 0.5251\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.36593\n",
      "Epoch 85/800\n",
      " - 41s - loss: 0.3881 - acc: 0.8627 - val_loss: 0.9551 - val_acc: 0.6532\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.36593\n",
      "Epoch 86/800\n",
      " - 42s - loss: 0.3876 - acc: 0.8627 - val_loss: 1.5688 - val_acc: 0.6580\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.36593\n",
      "Epoch 87/800\n",
      " - 42s - loss: 0.3874 - acc: 0.8629 - val_loss: 1.2320 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.36593\n",
      "Epoch 88/800\n",
      " - 42s - loss: 0.3873 - acc: 0.8619 - val_loss: 1.1213 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.36593\n",
      "Epoch 89/800\n",
      " - 42s - loss: 0.3866 - acc: 0.8632 - val_loss: 1.1619 - val_acc: 0.6505\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.36593\n",
      "Epoch 90/800\n",
      " - 42s - loss: 0.3861 - acc: 0.8629 - val_loss: 1.1876 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.36593\n",
      "Epoch 91/800\n",
      " - 42s - loss: 0.3861 - acc: 0.8623 - val_loss: 0.8560 - val_acc: 0.6715\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.36593\n",
      "Epoch 92/800\n",
      " - 41s - loss: 0.3856 - acc: 0.8627 - val_loss: 1.5564 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.36593\n",
      "Epoch 93/800\n",
      " - 41s - loss: 0.3860 - acc: 0.8623 - val_loss: 2.5093 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.36593\n",
      "Epoch 94/800\n",
      " - 41s - loss: 0.3858 - acc: 0.8628 - val_loss: 1.5960 - val_acc: 0.6059\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.36593\n",
      "Epoch 95/800\n",
      " - 42s - loss: 0.3859 - acc: 0.8628 - val_loss: 1.4386 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.36593\n",
      "Epoch 96/800\n",
      " - 42s - loss: 0.3864 - acc: 0.8623 - val_loss: 1.0258 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.36593\n",
      "Epoch 97/800\n",
      " - 42s - loss: 0.3856 - acc: 0.8625 - val_loss: 1.0972 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.36593\n",
      "Epoch 98/800\n",
      " - 44s - loss: 0.3849 - acc: 0.8628 - val_loss: 1.3006 - val_acc: 0.5542\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.36593\n",
      "Epoch 99/800\n",
      " - 45s - loss: 0.3839 - acc: 0.8637 - val_loss: 1.3269 - val_acc: 0.5589\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.36593\n",
      "Epoch 100/800\n",
      " - 45s - loss: 0.3840 - acc: 0.8631 - val_loss: 1.3098 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.36593\n",
      "Epoch 101/800\n",
      " - 45s - loss: 0.3838 - acc: 0.8638 - val_loss: 1.2389 - val_acc: 0.6588\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.36593\n",
      "Epoch 102/800\n",
      " - 45s - loss: 0.3833 - acc: 0.8632 - val_loss: 1.4154 - val_acc: 0.6292\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.36593\n",
      "Epoch 103/800\n",
      " - 46s - loss: 0.3815 - acc: 0.8643 - val_loss: 1.4123 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.36593\n",
      "Epoch 104/800\n",
      " - 45s - loss: 0.3829 - acc: 0.8635 - val_loss: 1.9162 - val_acc: 0.5568\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.36593\n",
      "Epoch 105/800\n",
      " - 45s - loss: 0.3810 - acc: 0.8648 - val_loss: 2.1402 - val_acc: 0.5264\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.36593\n",
      "Epoch 106/800\n",
      " - 41s - loss: 0.3820 - acc: 0.8643 - val_loss: 2.4950 - val_acc: 0.4861\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.36593\n",
      "Epoch 107/800\n",
      " - 44s - loss: 0.3794 - acc: 0.8645 - val_loss: 1.0335 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.36593\n",
      "Epoch 108/800\n",
      " - 45s - loss: 0.3777 - acc: 0.8654 - val_loss: 0.7284 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.36593\n",
      "Epoch 109/800\n",
      " - 46s - loss: 0.3786 - acc: 0.8656 - val_loss: 1.0381 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.36593\n",
      "Epoch 110/800\n",
      " - 45s - loss: 0.3777 - acc: 0.8664 - val_loss: 1.1282 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.36593\n",
      "Epoch 111/800\n",
      " - 45s - loss: 0.3761 - acc: 0.8660 - val_loss: 1.1109 - val_acc: 0.6345\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.36593\n",
      "Epoch 112/800\n",
      " - 45s - loss: 0.3753 - acc: 0.8665 - val_loss: 0.9512 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.36593\n",
      "Epoch 113/800\n",
      " - 45s - loss: 0.3747 - acc: 0.8662 - val_loss: 1.3584 - val_acc: 0.5965\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.36593\n",
      "Epoch 114/800\n",
      " - 44s - loss: 0.3737 - acc: 0.8661 - val_loss: 1.2830 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.36593\n",
      "Epoch 115/800\n",
      " - 45s - loss: 0.3722 - acc: 0.8674 - val_loss: 1.7773 - val_acc: 0.6024\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.36593\n",
      "Epoch 116/800\n",
      " - 45s - loss: 0.3716 - acc: 0.8670 - val_loss: 0.8556 - val_acc: 0.7029\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.36593\n",
      "Epoch 117/800\n",
      " - 45s - loss: 0.3717 - acc: 0.8668 - val_loss: 1.9920 - val_acc: 0.5951\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.36593\n",
      "Epoch 118/800\n",
      " - 45s - loss: 0.3693 - acc: 0.8684 - val_loss: 1.2690 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.36593\n",
      "Epoch 119/800\n",
      " - 45s - loss: 0.3699 - acc: 0.8676 - val_loss: 0.9591 - val_acc: 0.6957\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.36593\n",
      "Epoch 120/800\n",
      " - 42s - loss: 0.3667 - acc: 0.8687 - val_loss: 1.7327 - val_acc: 0.4956\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.36593\n",
      "Epoch 121/800\n",
      " - 45s - loss: 0.3676 - acc: 0.8683 - val_loss: 1.0466 - val_acc: 0.6779\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.36593\n",
      "Epoch 122/800\n",
      " - 45s - loss: 0.3655 - acc: 0.8685 - val_loss: 1.6753 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.36593\n",
      "Epoch 123/800\n",
      " - 45s - loss: 0.3658 - acc: 0.8688 - val_loss: 1.3208 - val_acc: 0.6264\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.36593\n",
      "Epoch 124/800\n",
      " - 45s - loss: 0.3630 - acc: 0.8699 - val_loss: 0.6641 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.36593\n",
      "Epoch 125/800\n",
      " - 45s - loss: 0.3631 - acc: 0.8700 - val_loss: 0.8193 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.36593\n",
      "Epoch 126/800\n",
      " - 45s - loss: 0.3621 - acc: 0.8701 - val_loss: 0.6963 - val_acc: 0.7794\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.36593\n",
      "Epoch 127/800\n",
      " - 45s - loss: 0.3608 - acc: 0.8704 - val_loss: 0.8770 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.36593\n",
      "Epoch 128/800\n",
      " - 45s - loss: 0.3596 - acc: 0.8710 - val_loss: 0.6771 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.36593\n",
      "Epoch 129/800\n",
      " - 45s - loss: 0.3571 - acc: 0.8714 - val_loss: 1.0515 - val_acc: 0.6581\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.36593\n",
      "Epoch 130/800\n",
      " - 45s - loss: 0.3573 - acc: 0.8717 - val_loss: 0.6594 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.36593\n",
      "Epoch 131/800\n",
      " - 45s - loss: 0.3570 - acc: 0.8722 - val_loss: 0.9637 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.36593\n",
      "Epoch 132/800\n",
      " - 45s - loss: 0.3555 - acc: 0.8723 - val_loss: 0.7322 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.36593\n",
      "Epoch 133/800\n",
      " - 45s - loss: 0.3544 - acc: 0.8731 - val_loss: 0.9568 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.36593\n",
      "Epoch 134/800\n",
      " - 45s - loss: 0.3539 - acc: 0.8727 - val_loss: 0.6142 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.36593\n",
      "Epoch 135/800\n",
      " - 45s - loss: 0.3537 - acc: 0.8727 - val_loss: 1.1204 - val_acc: 0.6485\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.36593\n",
      "Epoch 136/800\n",
      " - 45s - loss: 0.3516 - acc: 0.8733 - val_loss: 0.4215 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.36593\n",
      "Epoch 137/800\n",
      " - 45s - loss: 0.3518 - acc: 0.8737 - val_loss: 0.4327 - val_acc: 0.8436\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.36593\n",
      "Epoch 138/800\n",
      " - 45s - loss: 0.3499 - acc: 0.8739 - val_loss: 0.4778 - val_acc: 0.8308\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.36593\n",
      "Epoch 139/800\n",
      " - 45s - loss: 0.3487 - acc: 0.8744 - val_loss: 0.4400 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.36593\n",
      "Epoch 140/800\n",
      " - 45s - loss: 0.3484 - acc: 0.8742 - val_loss: 0.5838 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.36593\n",
      "Epoch 141/800\n",
      " - 46s - loss: 0.3480 - acc: 0.8747 - val_loss: 0.4035 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.36593\n",
      "Epoch 142/800\n",
      " - 45s - loss: 0.3479 - acc: 0.8747 - val_loss: 0.4438 - val_acc: 0.8440\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.36593\n",
      "Epoch 143/800\n",
      " - 45s - loss: 0.3476 - acc: 0.8744 - val_loss: 0.3783 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.36593\n",
      "Epoch 144/800\n",
      " - 45s - loss: 0.3467 - acc: 0.8746 - val_loss: 0.3777 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.36593\n",
      "Epoch 145/800\n",
      " - 45s - loss: 0.3457 - acc: 0.8760 - val_loss: 0.3814 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.36593\n",
      "Epoch 146/800\n",
      " - 45s - loss: 0.3458 - acc: 0.8758 - val_loss: 0.3892 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.36593\n",
      "Epoch 147/800\n",
      " - 45s - loss: 0.3458 - acc: 0.8757 - val_loss: 0.3934 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.36593\n",
      "Epoch 148/800\n",
      " - 45s - loss: 0.3451 - acc: 0.8752 - val_loss: 0.3859 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.36593\n",
      "Epoch 149/800\n",
      " - 45s - loss: 0.3463 - acc: 0.8755 - val_loss: 0.3785 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.36593\n",
      "Epoch 150/800\n",
      " - 45s - loss: 0.3450 - acc: 0.8763 - val_loss: 0.3767 - val_acc: 0.8695\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.36593\n",
      "Epoch 151/800\n",
      " - 45s - loss: 0.3457 - acc: 0.8750 - val_loss: 0.3786 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.36593\n",
      "Epoch 152/800\n",
      " - 45s - loss: 0.3447 - acc: 0.8755 - val_loss: 0.3790 - val_acc: 0.8702\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.36593\n",
      "Epoch 153/800\n",
      " - 45s - loss: 0.3442 - acc: 0.8756 - val_loss: 0.3828 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.36593\n",
      "Epoch 154/800\n",
      " - 45s - loss: 0.3452 - acc: 0.8754 - val_loss: 0.3674 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.36593\n",
      "Epoch 155/800\n",
      " - 45s - loss: 0.3440 - acc: 0.8752 - val_loss: 0.3803 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.36593\n",
      "Epoch 156/800\n",
      " - 45s - loss: 0.3459 - acc: 0.8747 - val_loss: 0.4069 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.36593\n",
      "Epoch 157/800\n",
      " - 45s - loss: 0.3442 - acc: 0.8755 - val_loss: 0.3962 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.36593\n",
      "Epoch 158/800\n",
      " - 45s - loss: 0.3455 - acc: 0.8753 - val_loss: 0.4044 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.36593\n",
      "Epoch 159/800\n",
      " - 45s - loss: 0.3445 - acc: 0.8759 - val_loss: 0.4619 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.36593\n",
      "Epoch 160/800\n",
      " - 46s - loss: 0.3467 - acc: 0.8747 - val_loss: 0.4848 - val_acc: 0.8337\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.36593\n",
      "Epoch 161/800\n",
      " - 46s - loss: 0.3465 - acc: 0.8751 - val_loss: 0.4121 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.36593\n",
      "Epoch 162/800\n",
      " - 46s - loss: 0.3457 - acc: 0.8755 - val_loss: 0.4360 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.36593\n",
      "Epoch 163/800\n",
      " - 45s - loss: 0.3470 - acc: 0.8751 - val_loss: 0.7317 - val_acc: 0.7237\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.36593\n",
      "Epoch 164/800\n",
      " - 46s - loss: 0.3477 - acc: 0.8745 - val_loss: 0.7564 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.36593\n",
      "Epoch 165/800\n",
      " - 46s - loss: 0.3475 - acc: 0.8744 - val_loss: 0.5174 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.36593\n",
      "Epoch 166/800\n",
      " - 46s - loss: 0.3493 - acc: 0.8747 - val_loss: 0.4229 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.36593\n",
      "Epoch 167/800\n",
      " - 45s - loss: 0.3491 - acc: 0.8736 - val_loss: 0.4631 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.36593\n",
      "Epoch 168/800\n",
      " - 45s - loss: 0.3488 - acc: 0.8736 - val_loss: 0.5020 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.36593\n",
      "Epoch 169/800\n",
      " - 46s - loss: 0.3495 - acc: 0.8735 - val_loss: 0.6225 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.36593\n",
      "Epoch 170/800\n",
      " - 46s - loss: 0.3512 - acc: 0.8732 - val_loss: 0.5403 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.36593\n",
      "Epoch 171/800\n",
      " - 45s - loss: 0.3506 - acc: 0.8732 - val_loss: 0.8323 - val_acc: 0.7362\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.36593\n",
      "Epoch 172/800\n",
      " - 45s - loss: 0.3520 - acc: 0.8733 - val_loss: 0.5234 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.36593\n",
      "Epoch 173/800\n",
      " - 46s - loss: 0.3530 - acc: 0.8726 - val_loss: 0.4238 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.36593\n",
      "Epoch 174/800\n",
      " - 46s - loss: 0.3525 - acc: 0.8720 - val_loss: 0.5071 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.36593\n",
      "Epoch 175/800\n",
      " - 46s - loss: 0.3538 - acc: 0.8725 - val_loss: 0.9103 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.36593\n",
      "Epoch 176/800\n",
      " - 46s - loss: 0.3558 - acc: 0.8724 - val_loss: 1.6506 - val_acc: 0.6212\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.36593\n",
      "Epoch 177/800\n",
      " - 46s - loss: 0.3547 - acc: 0.8727 - val_loss: 1.4205 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.36593\n",
      "Epoch 178/800\n",
      " - 45s - loss: 0.3547 - acc: 0.8722 - val_loss: 1.4401 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.36593\n",
      "Epoch 179/800\n",
      " - 46s - loss: 0.3558 - acc: 0.8709 - val_loss: 1.2790 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.36593\n",
      "Epoch 180/800\n",
      " - 45s - loss: 0.3579 - acc: 0.8713 - val_loss: 1.1581 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.36593\n",
      "Epoch 181/800\n",
      " - 46s - loss: 0.3564 - acc: 0.8718 - val_loss: 0.7483 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.36593\n",
      "Epoch 182/800\n",
      " - 46s - loss: 0.3572 - acc: 0.8713 - val_loss: 0.8168 - val_acc: 0.7335\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.36593\n",
      "Epoch 183/800\n",
      " - 46s - loss: 0.3581 - acc: 0.8710 - val_loss: 1.4555 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.36593\n",
      "Epoch 184/800\n",
      " - 45s - loss: 0.3583 - acc: 0.8708 - val_loss: 0.8871 - val_acc: 0.7348\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.36593\n",
      "Epoch 185/800\n",
      " - 45s - loss: 0.3602 - acc: 0.8707 - val_loss: 0.7438 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.36593\n",
      "Epoch 186/800\n",
      " - 45s - loss: 0.3587 - acc: 0.8708 - val_loss: 0.5434 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.36593\n",
      "Epoch 187/800\n",
      " - 46s - loss: 0.3602 - acc: 0.8706 - val_loss: 1.4614 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.36593\n",
      "Epoch 188/800\n",
      " - 46s - loss: 0.3594 - acc: 0.8707 - val_loss: 1.0838 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.36593\n",
      "Epoch 189/800\n",
      " - 45s - loss: 0.3621 - acc: 0.8697 - val_loss: 1.6233 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.36593\n",
      "Epoch 190/800\n",
      " - 46s - loss: 0.3606 - acc: 0.8705 - val_loss: 1.4815 - val_acc: 0.6278\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.36593\n",
      "Epoch 191/800\n",
      " - 46s - loss: 0.3615 - acc: 0.8708 - val_loss: 0.9884 - val_acc: 0.7096\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.36593\n",
      "Epoch 192/800\n",
      " - 46s - loss: 0.3624 - acc: 0.8694 - val_loss: 1.0797 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.36593\n",
      "Epoch 193/800\n",
      " - 47s - loss: 0.3624 - acc: 0.8702 - val_loss: 0.7500 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.36593\n",
      "Epoch 194/800\n",
      " - 43s - loss: 0.3617 - acc: 0.8689 - val_loss: 0.9859 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.36593\n",
      "Epoch 195/800\n",
      " - 45s - loss: 0.3604 - acc: 0.8706 - val_loss: 1.5609 - val_acc: 0.6273\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.36593\n",
      "Epoch 196/800\n",
      " - 48s - loss: 0.3608 - acc: 0.8702 - val_loss: 1.1839 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.36593\n",
      "Epoch 197/800\n",
      " - 49s - loss: 0.3617 - acc: 0.8696 - val_loss: 1.2764 - val_acc: 0.6346\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.36593\n",
      "Epoch 198/800\n",
      " - 49s - loss: 0.3615 - acc: 0.8699 - val_loss: 1.5408 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.36593\n",
      "Epoch 199/800\n",
      " - 50s - loss: 0.3626 - acc: 0.8702 - val_loss: 1.1543 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.36593\n",
      "Epoch 200/800\n",
      " - 49s - loss: 0.3622 - acc: 0.8702 - val_loss: 1.3137 - val_acc: 0.6555\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.36593\n",
      "Epoch 201/800\n",
      " - 49s - loss: 0.3618 - acc: 0.8702 - val_loss: 1.6355 - val_acc: 0.5699\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.36593\n",
      "Epoch 202/800\n",
      " - 49s - loss: 0.3617 - acc: 0.8697 - val_loss: 0.5969 - val_acc: 0.8074\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.36593\n",
      "Epoch 203/800\n",
      " - 49s - loss: 0.3597 - acc: 0.8709 - val_loss: 1.2078 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.36593\n",
      "Epoch 204/800\n",
      " - 49s - loss: 0.3595 - acc: 0.8705 - val_loss: 1.2398 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.36593\n",
      "Epoch 205/800\n",
      " - 46s - loss: 0.3598 - acc: 0.8704 - val_loss: 1.3520 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.36593\n",
      "Epoch 206/800\n",
      " - 51s - loss: 0.3581 - acc: 0.8715 - val_loss: 1.1080 - val_acc: 0.6815\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.36593\n",
      "Epoch 207/800\n",
      " - 51s - loss: 0.3594 - acc: 0.8709 - val_loss: 1.2186 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.36593\n",
      "Epoch 208/800\n",
      " - 51s - loss: 0.3574 - acc: 0.8714 - val_loss: 1.2920 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.36593\n",
      "Epoch 209/800\n",
      " - 51s - loss: 0.3573 - acc: 0.8718 - val_loss: 0.8932 - val_acc: 0.7301\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.36593\n",
      "Epoch 210/800\n",
      " - 51s - loss: 0.3564 - acc: 0.8716 - val_loss: 0.7273 - val_acc: 0.7494\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.36593\n",
      "Epoch 211/800\n",
      " - 50s - loss: 0.3554 - acc: 0.8713 - val_loss: 1.8237 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.36593\n",
      "Epoch 212/800\n",
      " - 51s - loss: 0.3561 - acc: 0.8715 - val_loss: 1.0131 - val_acc: 0.7101\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.36593\n",
      "Epoch 213/800\n",
      " - 51s - loss: 0.3550 - acc: 0.8716 - val_loss: 1.2466 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.36593\n",
      "Epoch 214/800\n",
      " - 51s - loss: 0.3553 - acc: 0.8718 - val_loss: 1.3856 - val_acc: 0.6102\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.36593\n",
      "Epoch 215/800\n",
      " - 50s - loss: 0.3544 - acc: 0.8720 - val_loss: 0.9330 - val_acc: 0.7256\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.36593\n",
      "Epoch 216/800\n",
      " - 51s - loss: 0.3525 - acc: 0.8720 - val_loss: 1.1922 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.36593\n",
      "Epoch 217/800\n",
      " - 50s - loss: 0.3515 - acc: 0.8723 - val_loss: 0.7124 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.36593\n",
      "Epoch 218/800\n",
      " - 50s - loss: 0.3510 - acc: 0.8727 - val_loss: 0.8085 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.36593\n",
      "Epoch 219/800\n",
      " - 51s - loss: 0.3495 - acc: 0.8732 - val_loss: 1.3875 - val_acc: 0.6279\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.36593\n",
      "Epoch 220/800\n",
      " - 51s - loss: 0.3493 - acc: 0.8742 - val_loss: 0.9965 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.36593\n",
      "Epoch 221/800\n",
      " - 51s - loss: 0.3499 - acc: 0.8737 - val_loss: 1.6437 - val_acc: 0.6217\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.36593\n",
      "Epoch 222/800\n",
      " - 51s - loss: 0.3470 - acc: 0.8742 - val_loss: 1.3834 - val_acc: 0.6278\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.36593\n",
      "Epoch 223/800\n",
      " - 51s - loss: 0.3447 - acc: 0.8748 - val_loss: 1.5766 - val_acc: 0.6298\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.36593\n",
      "Epoch 224/800\n",
      " - 50s - loss: 0.3454 - acc: 0.8748 - val_loss: 0.6679 - val_acc: 0.7894\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.36593\n",
      "Epoch 225/800\n",
      " - 50s - loss: 0.3442 - acc: 0.8752 - val_loss: 1.1975 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.36593\n",
      "Epoch 226/800\n",
      " - 50s - loss: 0.3430 - acc: 0.8745 - val_loss: 0.6606 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.36593\n",
      "Epoch 227/800\n",
      " - 51s - loss: 0.3434 - acc: 0.8754 - val_loss: 0.7971 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.36593\n",
      "Epoch 228/800\n",
      " - 51s - loss: 0.3422 - acc: 0.8764 - val_loss: 0.4966 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.36593\n",
      "Epoch 229/800\n",
      " - 50s - loss: 0.3393 - acc: 0.8768 - val_loss: 1.0005 - val_acc: 0.7018\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.36593\n",
      "Epoch 230/800\n",
      " - 50s - loss: 0.3396 - acc: 0.8766 - val_loss: 0.8411 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.36593\n",
      "Epoch 231/800\n",
      " - 51s - loss: 0.3392 - acc: 0.8764 - val_loss: 1.0728 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.36593\n",
      "Epoch 232/800\n",
      " - 50s - loss: 0.3373 - acc: 0.8773 - val_loss: 0.8650 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.36593\n",
      "Epoch 233/800\n",
      " - 51s - loss: 0.3378 - acc: 0.8772 - val_loss: 0.7829 - val_acc: 0.7404\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.36593\n",
      "Epoch 234/800\n",
      " - 50s - loss: 0.3356 - acc: 0.8773 - val_loss: 0.5506 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.36593\n",
      "Epoch 235/800\n",
      " - 50s - loss: 0.3353 - acc: 0.8779 - val_loss: 0.5953 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.36593\n",
      "Epoch 236/800\n",
      " - 51s - loss: 0.3344 - acc: 0.8787 - val_loss: 0.5156 - val_acc: 0.8211\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.36593\n",
      "Epoch 237/800\n",
      " - 51s - loss: 0.3332 - acc: 0.8791 - val_loss: 0.5283 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.36593\n",
      "Epoch 238/800\n",
      " - 51s - loss: 0.3329 - acc: 0.8794 - val_loss: 0.5435 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.36593\n",
      "Epoch 239/800\n",
      " - 51s - loss: 0.3316 - acc: 0.8789 - val_loss: 0.6725 - val_acc: 0.7787\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.36593\n",
      "Epoch 240/800\n",
      " - 51s - loss: 0.3321 - acc: 0.8787 - val_loss: 0.4029 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.36593\n",
      "Epoch 241/800\n",
      " - 51s - loss: 0.3319 - acc: 0.8791 - val_loss: 0.4280 - val_acc: 0.8546\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.36593\n",
      "Epoch 242/800\n",
      " - 51s - loss: 0.3303 - acc: 0.8797 - val_loss: 0.3926 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.36593\n",
      "Epoch 243/800\n",
      " - 50s - loss: 0.3305 - acc: 0.8794 - val_loss: 0.4090 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.36593\n",
      "Epoch 244/800\n",
      " - 51s - loss: 0.3287 - acc: 0.8808 - val_loss: 0.3974 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.36593\n",
      "Epoch 245/800\n",
      " - 50s - loss: 0.3287 - acc: 0.8794 - val_loss: 0.4317 - val_acc: 0.8538\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.36593\n",
      "Epoch 246/800\n",
      " - 51s - loss: 0.3280 - acc: 0.8806 - val_loss: 0.3825 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.36593\n",
      "Epoch 247/800\n",
      " - 49s - loss: 0.3287 - acc: 0.8805 - val_loss: 0.4153 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.36593\n",
      "Epoch 248/800\n",
      " - 48s - loss: 0.3283 - acc: 0.8802 - val_loss: 0.4015 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.36593\n",
      "Epoch 249/800\n",
      " - 48s - loss: 0.3284 - acc: 0.8798 - val_loss: 0.4116 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.36593\n",
      "Epoch 250/800\n",
      " - 48s - loss: 0.3281 - acc: 0.8807 - val_loss: 0.3931 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.36593\n",
      "Epoch 251/800\n",
      " - 44s - loss: 0.3272 - acc: 0.8809 - val_loss: 0.3856 - val_acc: 0.8677\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.36593\n",
      "Epoch 252/800\n",
      " - 38s - loss: 0.3284 - acc: 0.8807 - val_loss: 0.3787 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.36593\n",
      "Epoch 253/800\n",
      " - 39s - loss: 0.3278 - acc: 0.8804 - val_loss: 0.4176 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.36593\n",
      "Epoch 254/800\n",
      " - 43s - loss: 0.3279 - acc: 0.8800 - val_loss: 0.3777 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.36593\n",
      "Epoch 255/800\n",
      " - 44s - loss: 0.3277 - acc: 0.8802 - val_loss: 0.4148 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.36593\n",
      "Epoch 256/800\n",
      " - 42s - loss: 0.3276 - acc: 0.8802 - val_loss: 0.4100 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.36593\n",
      "Epoch 257/800\n",
      " - 42s - loss: 0.3293 - acc: 0.8800 - val_loss: 0.3951 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.36593\n",
      "Epoch 258/800\n",
      " - 43s - loss: 0.3286 - acc: 0.8797 - val_loss: 0.4028 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.36593\n",
      "Epoch 259/800\n",
      " - 44s - loss: 0.3277 - acc: 0.8807 - val_loss: 0.4210 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.36593\n",
      "Epoch 260/800\n",
      " - 43s - loss: 0.3287 - acc: 0.8805 - val_loss: 0.5157 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.36593\n",
      "Epoch 261/800\n",
      " - 43s - loss: 0.3298 - acc: 0.8797 - val_loss: 0.4272 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.36593\n",
      "Epoch 262/800\n",
      " - 43s - loss: 0.3293 - acc: 0.8796 - val_loss: 0.3865 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.36593\n",
      "Epoch 263/800\n",
      " - 44s - loss: 0.3302 - acc: 0.8794 - val_loss: 0.5090 - val_acc: 0.8331\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.36593\n",
      "Epoch 264/800\n",
      " - 44s - loss: 0.3312 - acc: 0.8791 - val_loss: 0.5362 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.36593\n",
      "Epoch 265/800\n",
      " - 43s - loss: 0.3305 - acc: 0.8795 - val_loss: 0.4474 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.36593\n",
      "Epoch 266/800\n",
      " - 43s - loss: 0.3311 - acc: 0.8795 - val_loss: 0.9403 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.36593\n",
      "Epoch 267/800\n",
      " - 43s - loss: 0.3327 - acc: 0.8790 - val_loss: 0.5335 - val_acc: 0.8169\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.36593\n",
      "Epoch 268/800\n",
      " - 43s - loss: 0.3331 - acc: 0.8785 - val_loss: 0.5336 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.36593\n",
      "Epoch 269/800\n",
      " - 43s - loss: 0.3321 - acc: 0.8789 - val_loss: 1.5072 - val_acc: 0.6326\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.36593\n",
      "Epoch 270/800\n",
      " - 44s - loss: 0.3332 - acc: 0.8785 - val_loss: 0.5041 - val_acc: 0.8305\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.36593\n",
      "Epoch 271/800\n",
      " - 43s - loss: 0.3355 - acc: 0.8782 - val_loss: 0.6359 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.36593\n",
      "Epoch 272/800\n",
      " - 43s - loss: 0.3342 - acc: 0.8780 - val_loss: 0.7821 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.36593\n",
      "Epoch 273/800\n",
      " - 44s - loss: 0.3350 - acc: 0.8783 - val_loss: 0.4701 - val_acc: 0.8446\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.36593\n",
      "Epoch 274/800\n",
      " - 45s - loss: 0.3355 - acc: 0.8778 - val_loss: 0.4945 - val_acc: 0.8395\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.36593\n",
      "Epoch 275/800\n",
      " - 44s - loss: 0.3366 - acc: 0.8774 - val_loss: 0.5388 - val_acc: 0.8328\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.36593\n",
      "Epoch 276/800\n",
      " - 45s - loss: 0.3376 - acc: 0.8773 - val_loss: 0.5260 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.36593\n",
      "Epoch 277/800\n",
      " - 44s - loss: 0.3385 - acc: 0.8765 - val_loss: 0.5681 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.36593\n",
      "Epoch 278/800\n",
      " - 44s - loss: 0.3387 - acc: 0.8763 - val_loss: 1.6439 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.36593\n",
      "Epoch 279/800\n",
      " - 45s - loss: 0.3386 - acc: 0.8771 - val_loss: 1.3972 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.36593\n",
      "Epoch 280/800\n",
      " - 45s - loss: 0.3401 - acc: 0.8767 - val_loss: 1.1620 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.36593\n",
      "Epoch 281/800\n",
      " - 44s - loss: 0.3386 - acc: 0.8778 - val_loss: 1.2639 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.36593\n",
      "Epoch 282/800\n",
      " - 45s - loss: 0.3406 - acc: 0.8762 - val_loss: 0.6075 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.36593\n",
      "Epoch 283/800\n"
     ]
    }
   ],
   "source": [
    "for enum, (train_index,valid_index) in enumerate(skf.split(x,y)):\n",
    "    \n",
    "    file_path = f\"/home/lab21/20200220/model/best_ensembl2_{enum}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    cosine_scheduler = CosineAnnealingScheduler(T_max=50, eta_max=6e-4, eta_min=3e-5)\n",
    "    swa = SWA(start_epoch=50, lr_schedule='manual', swa_lr=3e-4,  verbose=1,batch_size=256)\n",
    "\n",
    "      \n",
    "    skf_x_tr = x[train_index]\n",
    "    skf_y_tr = y[train_index]\n",
    "    \n",
    "    skf_x_val = x[valid_index]\n",
    "    skf_y_val = y[valid_index]\n",
    "\n",
    "    inps= Input(shape = (20,))\n",
    "    la = Dense(128)(inps)\n",
    "    la = BatchNormalization()(la)\n",
    "    la = Activation(gelu)(la)\n",
    "    x = Dropout(drop)(la)\n",
    "\n",
    "    for i in range(3):\n",
    "        xp = Dense(128)(x)\n",
    "\n",
    "        x = Dense(256)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(gelu)(x)\n",
    "        x= Dropout(0.25)(x)\n",
    "\n",
    "        xq= Dense(128)(x)\n",
    "\n",
    "        x= Concatenate()([xp,xq])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(gelu)(x)\n",
    "\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    \n",
    "    outs=Dense(19,activation='softmax',name='output')(x)\n",
    "\n",
    "    models = Model(inputs=inps, outputs=outs)\n",
    "    \n",
    "    \n",
    "    models.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=lr,decay=lr_d),metrics=['accuracy'])\n",
    "    models.fit(skf_x_tr, skf_y_tr, batch_size=256, epochs=nepoch, validation_data = [skf_x_val, skf_y_val],\n",
    "              verbose=2, callbacks=[early_stop,check_point,cosine_scheduler,swa])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=datetime.datetime.now()\n",
    "print(\"걸린 시간:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.zeros((len(test_x),19))\n",
    "\n",
    "for i in range(skfold):\n",
    "    print(i)    \n",
    "    model_load = load_model(\"/home/lab21/20200220/model/best_ensemble2_{}.hdf5\".format(i))\n",
    "    pred = np.array(model_load.predict(test_x))\n",
    "    pred_test += pred\n",
    "    \n",
    "pred_test = pred_test /skfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, stratify=y, train_size=0.8, \\\n",
    "                                                    shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc= models.evaluate(x_test,y_test)\n",
    "print(\"LOSS: \",loss, \"ACC:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame(data=pred_test, columns=sample_submission.columns, index=sample_submission.index)\n",
    "submission.to_csv('/home/lab21/20200220/csv/submission_ensemble2.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aws_neuron_tensorflow_p36]",
   "language": "python",
   "name": "conda-env-aws_neuron_tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
