{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "date='0222'\n",
    "filenumber='9'  #16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, Activation, BatchNormalization, Concatenate, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers, callbacks\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from swa.keras import SWA\n",
    "from keras import initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train = pd.read_csv('/home/lab21/data/train.csv', index_col=0)\n",
    "train_2 = pd.read_csv('/home/lab21/data/train_x_0.2_99.8.csv', index_col=0)\n",
    "\n",
    "test = pd.read_csv('/home/lab21/data/test.csv', index_col=0)\n",
    "sample_submission = pd.read_csv('/home/lab21/data/sample_submission.csv', index_col=0)\n",
    "\n",
    "# Train 데이터의 타입을 Sample_submission에 대응하는 가변수 형태로 변환\n",
    "column_number = {}\n",
    "for i, column in enumerate(sample_submission.columns):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "train['type_num'] = train['type'].apply(lambda x : to_number(x, column_number))\n",
    "\n",
    "# 모델에 적용할 데이터 셋 준비 \n",
    "#x = train.drop(columns=['type', 'type_num'], axis=1)\n",
    "y = train['type_num']\n",
    "\n",
    "x = train_2.drop(columns=['fiberID'], axis=1)\n",
    "\n",
    "test_x = test.drop(columns=['fiberID'],axis=1)\n",
    "\n",
    "x_name=x.columns\n",
    "col_name=x_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "test_x=np.array(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.astype('float32')\n",
    "test_x=test_x.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train_test_split\n",
    "# In[7]:\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y, stratify=y, \\\n",
    "                                                    train_size=0.6, shuffle=True ,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# scaler1=RobustScaler()\n",
    "scaler2=StandardScaler()\n",
    "\n",
    "scaler2.fit(x_train)\n",
    "x_train=scaler2.transform(x_train)\n",
    "x_val=scaler2.transform(x_val)\n",
    "\n",
    "test_x =scaler2.transform(test_x)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateScheduler(Callback):\n",
    "    \"\"\"LearningRateScheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, T_max, eta_max, eta_min, decay=10000,verbose=0):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.decay = decay\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "\n",
    "        x1=1.\n",
    "        x2=1.-(2.*self.eta_min)/(self.eta_max+self.eta_min)\n",
    "        x3=x1+x2\n",
    "        \n",
    "        lr=self.eta_max/(x3)*(x1-x2*(np.cos(math.pi * np.mod(epoch,self.T_max) / (self.T_max) +np.pi))) * np.power(0.5,epoch/self.decay)\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: LearningRateSchedule setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "\n",
    "class Gelu(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Gelu, self).__init__(activation, **kwargs)\n",
    "        self.__name__='gelu'\n",
    "        \n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "get_custom_objects().update({'gelu': Gelu(gelu)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2.e-5\n",
    "# lr_d = 0\n",
    "patience =  40\n",
    "drop = 0.05\n",
    "nepoch = 240*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max=60\n",
    "eta_max=2.0e-5\n",
    "eta_min=2.0e-5\n",
    "dec=0.000000001\n",
    "decay=nepoch/dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-22 10:00:36.253268\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "learning_scheduler = LearningRateScheduler(T_max=T_max, eta_max=eta_max, eta_min=eta_min, decay=decay, verbose=1)\n",
    "\n",
    "import datetime\n",
    "start=datetime.datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1_train=x_train.reshape(-1,20,1)\n",
    "x1_val= x_val.reshape(-1,20,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lab21/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/aws_neuron_tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4, 128)       768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          2688        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          33024       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 256)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65664       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          16512       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           dense_8[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 256)          65792       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          32896       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          32896       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           dense_4[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           dense_11[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 256)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           activation_4[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          65664       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          16512       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 19)           2451        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 663,571\n",
      "Trainable params: 663,571\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inps1 = Input(shape = (20,1))\n",
    "inps2 = Input(shape = (20,))\n",
    "\n",
    "node = 128 # 128\n",
    "#############################################################\n",
    "\n",
    "la1 = Conv1D(128, 5, activation='relu',padding='same', strides=5)(inps1)\n",
    "#la1 = Conv1D(64, 5, activation='relu',padding='same')(la1)\n",
    "x1 = Flatten()(la1)\n",
    "\n",
    "for i in range(2):\n",
    "    xp = Dense(node)(x1)\n",
    "\n",
    "    x1 = Dense(node*2)(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = Dropout(0.20)(x1)\n",
    "\n",
    "    xq= Dense(node)(x1)\n",
    "\n",
    "    x1 = Concatenate()([xp,xq])\n",
    "    x1 = Activation('relu')(x1)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "la2 = Dense(node)(inps2)\n",
    "x2  = Activation('relu')(la2)\n",
    "\n",
    "for i in range(2):\n",
    "    xp = Dense(node)(x2)\n",
    "\n",
    "    x2 = Dense(node*2)(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "    x2 = Dropout(0.20)(x2)\n",
    "\n",
    "    xq= Dense(node)(x2)\n",
    "\n",
    "    x2 = Concatenate()([xp,xq])\n",
    "    x2 = Activation('relu')(x2)\n",
    "\n",
    "\n",
    "x4 = Concatenate()([x1,x2])\n",
    "\n",
    "x4 = Dense(node)(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = Dropout(drop)(x4)\n",
    "\n",
    "x4 = Dense(node)(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = Dropout(drop)(x4)\n",
    "\n",
    "outs=Dense(19,activation='softmax',name='output')(x4)\n",
    "\n",
    "models = Model(inputs=[inps1,inps2], outputs=outs)\n",
    "models.fit\n",
    "models.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr),metrics=['accuracy'])\n",
    "\n",
    "models.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lab21/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 119994 samples, validate on 79997 samples\n",
      "Epoch 1/960\n",
      "\n",
      "Epoch 00001: LearningRateSchedule setting learning rate to 2e-05.\n",
      " - 12s - loss: 1.8006 - acc: 0.5099 - val_loss: 1.0825 - val_acc: 0.6671\n",
      "Epoch 2/960\n",
      "\n",
      "Epoch 00002: LearningRateSchedule setting learning rate to 1.999999999998556e-05.\n",
      " - 10s - loss: 1.0072 - acc: 0.6849 - val_loss: 0.8705 - val_acc: 0.7387\n",
      "Epoch 3/960\n",
      "\n",
      "Epoch 00003: LearningRateSchedule setting learning rate to 1.999999999997112e-05.\n",
      " - 10s - loss: 0.8804 - acc: 0.7267 - val_loss: 0.7703 - val_acc: 0.7760\n",
      "Epoch 4/960\n",
      "\n",
      "Epoch 00004: LearningRateSchedule setting learning rate to 1.999999999995668e-05.\n",
      " - 10s - loss: 0.7836 - acc: 0.7640 - val_loss: 0.6869 - val_acc: 0.7976\n",
      "Epoch 5/960\n",
      "\n",
      "Epoch 00005: LearningRateSchedule setting learning rate to 1.9999999999942237e-05.\n",
      " - 11s - loss: 0.7110 - acc: 0.7856 - val_loss: 0.6314 - val_acc: 0.8124\n",
      "Epoch 6/960\n",
      "\n",
      "Epoch 00006: LearningRateSchedule setting learning rate to 1.99999999999278e-05.\n",
      " - 11s - loss: 0.6625 - acc: 0.7971 - val_loss: 0.5958 - val_acc: 0.8169\n",
      "Epoch 7/960\n",
      "\n",
      "Epoch 00007: LearningRateSchedule setting learning rate to 1.9999999999913357e-05.\n",
      " - 11s - loss: 0.6289 - acc: 0.8063 - val_loss: 0.5714 - val_acc: 0.8212\n",
      "Epoch 8/960\n",
      "\n",
      "Epoch 00008: LearningRateSchedule setting learning rate to 1.9999999999898917e-05.\n",
      " - 11s - loss: 0.6018 - acc: 0.8133 - val_loss: 0.5512 - val_acc: 0.8253\n",
      "Epoch 9/960\n",
      "\n",
      "Epoch 00009: LearningRateSchedule setting learning rate to 1.9999999999884477e-05.\n",
      " - 12s - loss: 0.5855 - acc: 0.8164 - val_loss: 0.5350 - val_acc: 0.8297\n",
      "Epoch 10/960\n",
      "\n",
      "Epoch 00010: LearningRateSchedule setting learning rate to 1.9999999999870036e-05.\n",
      " - 11s - loss: 0.5687 - acc: 0.8199 - val_loss: 0.5250 - val_acc: 0.8304\n",
      "Epoch 11/960\n",
      "\n",
      "Epoch 00011: LearningRateSchedule setting learning rate to 1.9999999999855596e-05.\n",
      " - 11s - loss: 0.5562 - acc: 0.8223 - val_loss: 0.5121 - val_acc: 0.8335\n",
      "Epoch 12/960\n",
      "\n",
      "Epoch 00012: LearningRateSchedule setting learning rate to 1.9999999999841156e-05.\n",
      " - 12s - loss: 0.5428 - acc: 0.8260 - val_loss: 0.5105 - val_acc: 0.8329\n",
      "Epoch 13/960\n",
      "\n",
      "Epoch 00013: LearningRateSchedule setting learning rate to 1.9999999999826716e-05.\n",
      " - 12s - loss: 0.5340 - acc: 0.8267 - val_loss: 0.4957 - val_acc: 0.8366\n",
      "Epoch 14/960\n",
      "\n",
      "Epoch 00014: LearningRateSchedule setting learning rate to 1.9999999999812275e-05.\n",
      " - 11s - loss: 0.5237 - acc: 0.8297 - val_loss: 0.4859 - val_acc: 0.8394\n",
      "Epoch 15/960\n",
      "\n",
      "Epoch 00015: LearningRateSchedule setting learning rate to 1.9999999999797835e-05.\n",
      " - 12s - loss: 0.5165 - acc: 0.8317 - val_loss: 0.4794 - val_acc: 0.8408\n",
      "Epoch 16/960\n",
      "\n",
      "Epoch 00016: LearningRateSchedule setting learning rate to 1.999999999978339e-05.\n",
      " - 12s - loss: 0.5083 - acc: 0.8325 - val_loss: 0.4743 - val_acc: 0.8416\n",
      "Epoch 17/960\n",
      "\n",
      "Epoch 00017: LearningRateSchedule setting learning rate to 1.9999999999768955e-05.\n",
      " - 11s - loss: 0.5036 - acc: 0.8336 - val_loss: 0.4711 - val_acc: 0.8421\n",
      "Epoch 18/960\n",
      "\n",
      "Epoch 00018: LearningRateSchedule setting learning rate to 1.999999999975451e-05.\n",
      " - 11s - loss: 0.4979 - acc: 0.8357 - val_loss: 0.4647 - val_acc: 0.8443\n",
      "Epoch 19/960\n",
      "\n",
      "Epoch 00019: LearningRateSchedule setting learning rate to 1.999999999974007e-05.\n",
      " - 11s - loss: 0.4928 - acc: 0.8368 - val_loss: 0.4613 - val_acc: 0.8449\n",
      "Epoch 20/960\n",
      "\n",
      "Epoch 00020: LearningRateSchedule setting learning rate to 1.999999999972563e-05.\n",
      " - 11s - loss: 0.4868 - acc: 0.8377 - val_loss: 0.4566 - val_acc: 0.8454\n",
      "Epoch 21/960\n",
      "\n",
      "Epoch 00021: LearningRateSchedule setting learning rate to 1.999999999971119e-05.\n",
      " - 11s - loss: 0.4843 - acc: 0.8379 - val_loss: 0.4545 - val_acc: 0.8459\n",
      "Epoch 22/960\n",
      "\n",
      "Epoch 00022: LearningRateSchedule setting learning rate to 1.999999999969675e-05.\n",
      " - 11s - loss: 0.4803 - acc: 0.8393 - val_loss: 0.4526 - val_acc: 0.8470\n",
      "Epoch 23/960\n",
      "\n",
      "Epoch 00023: LearningRateSchedule setting learning rate to 1.999999999968231e-05.\n",
      " - 11s - loss: 0.4752 - acc: 0.8406 - val_loss: 0.4497 - val_acc: 0.8466\n",
      "Epoch 24/960\n",
      "\n",
      "Epoch 00024: LearningRateSchedule setting learning rate to 1.999999999966787e-05.\n",
      " - 11s - loss: 0.4724 - acc: 0.8414 - val_loss: 0.4447 - val_acc: 0.8483\n",
      "Epoch 25/960\n",
      "\n",
      "Epoch 00025: LearningRateSchedule setting learning rate to 1.9999999999653426e-05.\n",
      " - 11s - loss: 0.4685 - acc: 0.8420 - val_loss: 0.4443 - val_acc: 0.8481\n",
      "Epoch 26/960\n",
      "\n",
      "Epoch 00026: LearningRateSchedule setting learning rate to 1.999999999963899e-05.\n",
      " - 11s - loss: 0.4673 - acc: 0.8422 - val_loss: 0.4420 - val_acc: 0.8488\n",
      "Epoch 27/960\n",
      "\n",
      "Epoch 00027: LearningRateSchedule setting learning rate to 1.9999999999624546e-05.\n",
      " - 11s - loss: 0.4646 - acc: 0.8428 - val_loss: 0.4411 - val_acc: 0.8496\n",
      "Epoch 28/960\n",
      "\n",
      "Epoch 00028: LearningRateSchedule setting learning rate to 1.9999999999610106e-05.\n",
      " - 12s - loss: 0.4615 - acc: 0.8440 - val_loss: 0.4398 - val_acc: 0.8500\n",
      "Epoch 29/960\n",
      "\n",
      "Epoch 00029: LearningRateSchedule setting learning rate to 1.9999999999595665e-05.\n",
      " - 11s - loss: 0.4587 - acc: 0.8457 - val_loss: 0.4374 - val_acc: 0.8511\n",
      "Epoch 30/960\n",
      "\n",
      "Epoch 00030: LearningRateSchedule setting learning rate to 1.9999999999581225e-05.\n",
      " - 12s - loss: 0.4567 - acc: 0.8444 - val_loss: 0.4357 - val_acc: 0.8514\n",
      "Epoch 31/960\n",
      "\n",
      "Epoch 00031: LearningRateSchedule setting learning rate to 1.9999999999566785e-05.\n",
      " - 11s - loss: 0.4540 - acc: 0.8457 - val_loss: 0.4338 - val_acc: 0.8519\n",
      "Epoch 32/960\n",
      "\n",
      "Epoch 00032: LearningRateSchedule setting learning rate to 1.9999999999552345e-05.\n",
      " - 11s - loss: 0.4524 - acc: 0.8467 - val_loss: 0.4319 - val_acc: 0.8528\n",
      "Epoch 33/960\n",
      "\n",
      "Epoch 00033: LearningRateSchedule setting learning rate to 1.9999999999537905e-05.\n",
      " - 12s - loss: 0.4510 - acc: 0.8467 - val_loss: 0.4350 - val_acc: 0.8514\n",
      "Epoch 34/960\n",
      "\n",
      "Epoch 00034: LearningRateSchedule setting learning rate to 1.9999999999523464e-05.\n",
      " - 11s - loss: 0.4496 - acc: 0.8476 - val_loss: 0.4288 - val_acc: 0.8539\n",
      "Epoch 35/960\n",
      "\n",
      "Epoch 00035: LearningRateSchedule setting learning rate to 1.9999999999509024e-05.\n",
      " - 11s - loss: 0.4475 - acc: 0.8477 - val_loss: 0.4275 - val_acc: 0.8539\n",
      "Epoch 36/960\n",
      "\n",
      "Epoch 00036: LearningRateSchedule setting learning rate to 1.999999999949458e-05.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hist = models.fit([x1_train,x_train], y_train, batch_size= 256, epochs=nepoch, verbose=2, validation_data=[[x1_val,x_val], y_val], callbacks=[early_stop,learning_scheduler]) #,swa\n",
    "\n",
    "loss, acc = models.evaluate([x1_test,x_test],y_test,batch_size= 256)\n",
    "print('Loss:',loss,'Accuracy:',acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=datetime.datetime.now()\n",
    "print(end)\n",
    "print(\"걸린 시간:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #y_pred = models.predict_proba(test_x)\n",
    "\n",
    "# test1_x=test_x.reshape(-1,20,1)\n",
    "# y_pred = models.predict([test1_x,test_x])\n",
    "\n",
    "# # 제출 파일 생성\n",
    "# submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
    "# submission.to_csv('/home/lab21/data/submission_no_fold_'+date+filenumber+'.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(hist.history.keys())\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='center')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='center right')\n",
    "\n",
    "plt.savefig('/home/lab21/data/submission_no_fold_'+date+filenumber+'_1.png')\n",
    "plt.show()\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='lower right')\n",
    "loss_ax.set_xlim(-1,1000)\n",
    "loss_ax.set_ylim(0.3,0.4)\n",
    "plt.savefig('/home/lab21/data/submission_no_fold_tr_'+date+filenumber+'_2.png')\n",
    "plt.show()\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='center right')\n",
    "acc_ax.set_xlim(100,1000)\n",
    "acc_ax.set_ylim(0.86,0.88)\n",
    "plt.savefig('/home/lab21/data/submission_no_fold_tr_'+date+filenumber+'_3.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['lr'])\n",
    "plt.savefig('/home/lab21/data/submission_no_fold_tr_'+date+filenumber+'_lr.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aws_neuron_tensorflow_p36]",
   "language": "python",
   "name": "conda-env-aws_neuron_tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
