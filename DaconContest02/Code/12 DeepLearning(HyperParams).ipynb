{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 불러오기\n",
    "# train = pd.read_csv('./data/train.csv', index_col=0)\n",
    "# test = pd.read_csv('./data/test.csv', index_col=0)\n",
    "# sample_submission = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "\n",
    "# # Train 데이터의 타입을 Sample_submission에 대응하는 가변수 형태로 변환\n",
    "# column_number = {}\n",
    "# for i, column in enumerate(sample_submission.columns):\n",
    "#     column_number[column] = i\n",
    "    \n",
    "# def to_number(x, dic):\n",
    "#     return dic[x]\n",
    "\n",
    "# train['type_num'] = train['type'].apply(lambda x : to_number(x, column_number))\n",
    "\n",
    "# # 모델에 적용할 데이터 셋 준비 \n",
    "# x = train.drop(columns=['type', 'type_num'], axis=1)\n",
    "# y = train['type_num']\n",
    "\n",
    "# x = x.drop(columns=['fiberID'], axis=1)\n",
    "# test_x = test.drop(columns=['fiberID'],axis=1)\n",
    "\n",
    "# x_name=x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.load('./data/x_percentile.npy')\n",
    "x = np.load('./data/x_try.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "\n",
    "y = np.load('./data/y.npy')\n",
    "pred = np.load('./data/pred.npy')\n",
    "yy =pd.read_csv('./data/yy.csv',header=None)\n",
    "\n",
    "path='./data/column_name.txt'\n",
    "with open(path,'r')  as f:\n",
    "    col_name=f.read() \n",
    "\n",
    "x_name=col_name.split('\\n')\n",
    "try:\n",
    "    del(x_name[20])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=np.array(x)\n",
    "#test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, stratify=y,train_size=0.8, shuffle=True ,random_state=0)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.8, shuffle=True ,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "scaler1=StandardScaler()\n",
    "#scaler1=RobustScaler()\n",
    "scaler1.fit(x_train)\n",
    "x_train=scaler1.transform(x_train)\n",
    "x_test=scaler1.transform(x_test)\n",
    "\n",
    "\n",
    "scaler2=StandardScaler()\n",
    "#scaler2=RobustScaler()\n",
    "scaler2.fit(test_x)\n",
    "test_x =scaler2.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 현재까지 가장 안정적인 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(init_mode='uniform',dropout_rate=0.5,learn_rate=0.01,neurons=128,optimizers='adam'):\n",
    "    \n",
    "    inputs=Input(shape=(20,), name='input')\n",
    "    \n",
    "    ### 4개의 층\n",
    "    \n",
    "    # 5개의 층\n",
    "\n",
    "    x=Dense(128, activation='elu', name='hidden1')(inputs)\n",
    "    x=Dense(neurons, activation='elu', name='hidden2', kernel_initializer=init_mode,\\\n",
    "           kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x=Dense(neurons, activation='elu', name='hidden3', kernel_initializer=init_mode,\\\n",
    "           kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x=Dense(neurons, activation='elu', name='hidden4',kernel_initializer=init_mode,\\\n",
    "           kernel_regularizer=regularizers.l2(0.01))(x)       \n",
    "    x=Dropout(dropout_rate)(x)\n",
    "    x=Dense(neurons, activation='elu', name='hidden5',kernel_initializer=init_mode,\\\n",
    "           kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    \n",
    "    prediction = Dense(19, activation='softmax', name='output')(x)\n",
    "    model = Model(inputs=inputs, outputs=prediction)\n",
    "    \n",
    " \n",
    "    model.compile(optimizer=optimizers, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_hyperparameters():\n",
    "    batches=[128,256,512]\n",
    "    dropout_rate = [0.25,0.5] #np.linspace(0.1, 0.5, 3)\n",
    "    epochs = [100, 130]\n",
    "    neurons = [128, 256]\n",
    "    init_mode = [ 'glorot_normal', 'he_normal']\n",
    "    \n",
    "    #return{\"batch_size\":batches, \"optimizer\":optimizers}\n",
    "\n",
    "    return{\"batch_size\":batches, \"dropout_rate\":dropout_rate, \"epochs\":epochs, \\\n",
    "            \"neurons\":neurons, \"init_mode\":init_mode}\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model=KerasClassifier(build_fn=build_network, verbose=1)\n",
    "\n",
    "hyperparameters= create_hyperparameters()\n",
    "\n",
    "#print(hyperparameters)\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "search = GridSearchCV(estimator=model, param_grid=hyperparameters, n_jobs=-1, cv=4, verbose=1)\n",
    "#search = RandomizedSearchCV(estimator=model, param_distributions=hyperparameters, n_jobs=-1, cv=4, verbose=1)\n",
    "\n",
    "search.fit(x_train, y_train)\n",
    "print(search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
