{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월간 데이콘 2 천체 유형 분류\n",
    "## Public 2위 Private 1위 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매달 재밌는 대회 개최해주셔서 감사합니다.\n",
    "모두 고생하셨습니다!\n",
    "\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "1. Magnitude, row별 max, min, max-min, std, sum Feature 추가\n",
    "\n",
    "2. 모든 magnitude들의 조합(2)으로 diff feature 추가 \n",
    "\n",
    "3. 각 magnitude 별 max-max, min-min, sum-sum 을 구함\n",
    "\n",
    "4. 정확히 이것이 무엇인지는 모르겠는데 ugriz를 다른 system으로 변환하는 것 같았습니다. (성능 차이 거의 없음)\n",
    "\n",
    "5. fiberID별 fiberMag mean, (fiber_u,g,r,i,z)/fiberMag_mean\n",
    "\n",
    "6. 아래 사이트를 읽어보고 icolor, scolor, p1 등 feature 추가\n",
    " -> https://www.sdss.org/dr16/algorithms/segue_target_selection/#Legacy\n",
    "\n",
    "7. asinh 변환\n",
    "\n",
    "8. 위에서 구한 diff feature들의 표준편차\n",
    "\n",
    "9. 원본 Magnitude들 decomposition 수행하여 Feature 추가\n",
    "\n",
    "10. Permutation Importance를 사용하여 Feature Selection\n",
    "\n",
    "\n",
    "Modeling\n",
    "\n",
    "1. LightGBM Single Model이었고 Parameter는 모두 Hyper Optimization으로 찾았습니다.\n",
    "\n",
    "2. 'boosting_type': 'dart' 로 한것이 효과가 좋았습니다. \n",
    " -> gbdt가 0.3285정도 나왔고 dart는 0.3255, goss는 0.3300 정도 나왔습니다.\n",
    "\n",
    "3. stratifiedkfold 5fold를 사용했고 stratified에 type을 넣었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission의 순서로 string으로 된 train type을 숫자로 변경\n",
    "# https://dacon.io/competitions/official/235573/codeshare/690\n",
    "column_number = {}\n",
    "for i, column in enumerate(submission.columns[1:]):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "train['type_num'] = train['type'].apply(lambda x: to_number(x, column_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199991, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering하면서 계속 사용하게 될 각 Magnitude column(u,g,r,i,z)을 list로 변경\n",
    "psfMag_col = [c for c in train.columns if c.find('psfMag')!=-1]\n",
    "fiberMag_col = [c for c in train.columns if c.find('fiberMag')!=-1]\n",
    "petroMag_col = [c for c in train.columns if c.find('petroMag')!=-1]\n",
    "modelMag_col = [c for c in train.columns if c.find('modelMag')!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip 함수를 이용하여 각 Row별, Magnitude별 max, min, max-min, std, sum을 구한다.\n",
    "# mean, skew, 등 다른 것들 시도 시 cv 점수가 안 좋아져서 사용하지 않음\n",
    "for prefix, g in zip(['psfMag','fiberMag','petroMag','modelMag'], [psfMag_col, fiberMag_col, petroMag_col, modelMag_col]):\n",
    "    train[f'{prefix}_max'] = train[g].max(axis=1)\n",
    "    test[f'{prefix}_max'] = test[g].max(axis=1)\n",
    "    \n",
    "    train[f'{prefix}_min'] = train[g].min(axis=1)\n",
    "    test[f'{prefix}_min'] = test[g].min(axis=1)\n",
    "    \n",
    "    train[f'{prefix}_diff'] = train[f'{prefix}_max'] - train[f'{prefix}_min']\n",
    "    test[f'{prefix}_diff'] = test[f'{prefix}_max'] - test[f'{prefix}_min']\n",
    "    \n",
    "    train[f'{prefix}_std'] = train[g].std(axis=1)\n",
    "    test[f'{prefix}_std'] = test[g].std(axis=1)\n",
    "    \n",
    "    train[f'{prefix}_sum'] = train[g].sum(axis=1)\n",
    "    test[f'{prefix}_sum'] = test[g].sum(axis=1)\n",
    "    \n",
    "    #print(train[g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 psfMag, fiberMag, petroMag, modelMag (u, g, r, i, z) \n",
    "\n",
    "- 최대 (4개)\n",
    "- 최소 (4개)\n",
    "- 최대-최소 (4개)\n",
    "- 표준편차 (4개)\n",
    "- 합 (4개)\n",
    "\n",
    "총 20개의 새로운 열 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199991, 44)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diff feature 추가 예: psfMag_z - psfMag_i \n",
    "- sdss lagacy solution 등을 보면 대 부분 mag간 차이를 사용하기 때문에 이런 diff feature가 의미가 있을 것이라고 판단\n",
    "- 그리고 각 magnitude에서만 diff를 구하는 것이 아닌 itertools combinations를 활용하여 전체 magnitude에서 diff를 구함\n",
    "- 총 190가지 조합이 나오고 여기서 안 좋은 것은 permutation importance를 활용하여 feature 제거 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(psfMag_col[::-1]+fiberMag_col[::-1]+petroMag_col[::-1]+modelMag_col[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b\n",
      "a c\n",
      "b c\n"
     ]
    }
   ],
   "source": [
    "for i,j in itertools.combinations(['a','b','c'], 2):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각각의 열 끼리 뺄셈 연산을 통해 새로운 190개열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_feature = []\n",
    "for c1, c2 in itertools.combinations(psfMag_col[::-1]+fiberMag_col[::-1]+petroMag_col[::-1]+modelMag_col[::-1],2):\n",
    "    #print(c1,c2)\n",
    "    new_c = f'{c1}_{c2}_diff'\n",
    "    train[new_c] = train[c1]-train[c2]\n",
    "    test[new_c] = test[c1]-test[c2]\n",
    "    diff_feature.append(new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199991, 234)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 magnitude 별로 최대값, 최소값, 합을 구한 후에 각각을 뺄셈연산을 통해 총 18개 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 magnitude 별 max-max, min-min, sum-sum 을 구함\n",
    "for c in itertools.combinations(['psfMag','fiberMag','petroMag','modelMag'],2):\n",
    "    train[f'{c[0]}_{c[1]}_max_diff'] = train[f'{c[0]}_max'] - train[f'{c[1]}_max']\n",
    "    test[f'{c[0]}_{c[1]}_max_diff'] = test[f'{c[0]}_max'] - test[f'{c[1]}_max']\n",
    "    \n",
    "    train[f'{c[0]}_{c[1]}_min_diff'] = train[f'{c[0]}_min'] - train[f'{c[1]}_min']\n",
    "    test[f'{c[0]}_{c[1]}_min_diff'] = test[f'{c[0]}_min'] - test[f'{c[1]}_min']\n",
    "    \n",
    "    train[f'{c[0]}_{c[1]}_sum_diff'] = train[f'{c[0]}_sum'] - train[f'{c[1]}_sum']\n",
    "    test[f'{c[0]}_{c[1]}_sum_diff'] = test[f'{c[0]}_sum'] - test[f'{c[1]}_sum']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199991, 252)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도메인 지식이 없어 정확히는 모르지만 아래와 같은 공식들이 있어 구현함\n",
    "# 추가적으로 다른 것들도 시도하였는데 좋아지지 않음\n",
    "# http://classic.sdss.org/dr4/algorithms/sdssUBVRITransform.html\n",
    "def make_2flux_feature(train, test, c1,c2, func, mag_list=None):\n",
    "\n",
    "    for c in mag_list:\n",
    "        x=train[f'{c}_{c1}'].values\n",
    "        y=train[f'{c}_{c2}'].values\n",
    "        train[f'{c}_{func.__name__}'] = func(x,y)\n",
    "        \n",
    "        x=test[f'{c}_{c1}'].values\n",
    "        y=test[f'{c}_{c2}'].values\n",
    "        \n",
    "        test[f'{c}_{func.__name__}'] = func(x,y)\n",
    "        \n",
    "def quasar_UB_jester(x1, x2):\n",
    "    return 0.75*(x1-x2)-0.81\n",
    "\n",
    "make_2flux_feature(train, test, 'u','g',quasar_UB_jester,['psfMag'])\n",
    "\n",
    "def quasar_BV_jester(x1, x2):\n",
    "    return 0.62*(x1-x2)+0.15\n",
    "\n",
    "make_2flux_feature(train, test, 'g','r',quasar_BV_jester,['psfMag'])\n",
    "\n",
    "def quasar_VR_jester(x1, x2):\n",
    "    return 0.38*(x1-x2)+0.27\n",
    "\n",
    "make_2flux_feature(train, test, 'r','i',quasar_VR_jester,['psfMag'])\n",
    "\n",
    "def quasar_RcIc_jester(x1, x2):\n",
    "    return 0.72*(x1-x2)+0.27\n",
    "\n",
    "make_2flux_feature(train, test, 'u','g',quasar_RcIc_jester,['psfMag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jungg\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "def groupby_helper(all_data, source, target, agg_func):\n",
    "    \n",
    "    temp = all_data.groupby(source)[target].agg(agg_func)\n",
    "    new_col = ['_'.join(source)+'_' +c[0]+'_'+c[1] for c in itertools.product(target,agg_func)]\n",
    "    temp.columns = new_col\n",
    "    temp = temp.reset_index()\n",
    "    all_data = all_data.merge(temp, on=source, how='left')\n",
    "    return all_data, new_col\n",
    "\n",
    "# fiberID별 fiber Magnitude 평균 aggregation 수행\n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data, new_c = groupby_helper(all_data, ['fiberID'], fiberMag_col, ['mean'])\n",
    "\n",
    "# 각 filter별 fiberMag/fiberMag_mean\n",
    "for c1, c2 in zip(new_c, fiberMag_col):\n",
    "    all_data[f'{c2}_div_mean'] = all_data[c2]/all_data[c1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### psfMag와 연관된 새로운 열 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sdss.org/dr16/algorithms/segue_target_selection/#Legacy\n",
    "all_data['psf_icolor'] = all_data['psfMag_u']*(-0.436) + all_data['psfMag_g']*(1.129) + all_data['psfMag_r']*(-0.119) + all_data['psfMag_i']*(-0.574) +0.1984\n",
    "all_data['psf_scolor'] = all_data['psfMag_u']*(-0.249) + all_data['psfMag_g']*(0.794) + all_data['psfMag_r']*(-0.555) +0.234\n",
    "all_data['psf_p1'] = (all_data['psfMag_u']-all_data['psfMag_g'])*(0.91) + (all_data['psfMag_g']-all_data['psfMag_r'])*(0.415) -1.280\n",
    "all_data['psfMag_r_std_div'] = all_data['psfMag_r']/all_data['psfMag_r'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210000, 270)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modelMag와 연관된 새로운 열 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sdss.org/dr16/algorithms/legacy_target_selection/\n",
    "all_data['modelMag_borthogonal'] =(all_data['modelMag_r']-all_data['modelMag_i'])-(all_data['modelMag_g']-all_data['modelMag_r'])/4-0.177\n",
    "all_data['modelMag_parallel'] = 0.7*(all_data['modelMag_g']-all_data['modelMag_r']) + 1.2*((all_data['modelMag_r']-all_data['modelMag_i'])-0.177)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210000, 272)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sdss.org/dr12/algorithms/magnitudes/\n",
    "color_list = ['u', 'g', 'r', 'i', 'z']\n",
    "b_list = [1.4*10e-10, 0.9*10e-10, 1.2*10e-10, 1.8*10e-10, 7.4*10e-10]\n",
    "f0_list = [24.63, 25.11, 24.80, 24.36, 22.83]\n",
    "for c, b, f0 in zip(color_list, b_list, f0_list):\n",
    "    all_data[f'psfMag_{c}_asinh'] = -2.5*np.log(10)*(np.arcsinh((all_data[f'psfMag_{c}']/f0)/(2*b))+np.log(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210000, 277)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# # from sklearn.inspection import permutation_importance  # sklearn 22 버전부터 해당\n",
    "\n",
    "# my_model=RandomForestClassifier().fit(train,test)\n",
    "# perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "# # eli5.show_weights(perm, feature_names = val_X.columns.tolist()) # Notebook에서 실행 가능\n",
    "\n",
    "# print(eli5.format_as_text(eli5.explain_weights(perm, feature_names = val_X.columns.tolist())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permutation importacne로 찾아낸 나쁜 feature들 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation importacne로 찾아낸 나쁜 feature들 제거\n",
    "bad_feature = ['petroMag_i_modelMag_i_diff','petroMag_g_modelMag_g_diff','psfMag_g_modelMag_g_diff',\n",
    " 'psfMag_g_petroMag_g_diff','psfMag_r_petroMag_r_diff','petroMag_modelMag_min_diff',\n",
    " 'psfMag_fiberMag_min_diff','psfMag_modelMag_min_diff','psfMag_fiberMag_sum_diff',\n",
    " 'psfMag_u_fiberMag_u_diff','psfMag_u_modelMag_u_diff','psfMag_modelMag_sum_diff',\n",
    " 'psfMag_z_fiberMag_z_diff','psfMag_petroMag_min_diff','psfMag_z_modelMag_z_diff',\n",
    " 'fiberMag_modelMag_sum_diff','psfMag_fiberMag_max_diff','psfMag_modelMag_max_diff',\n",
    " 'petroMag_z_modelMag_z_diff','fiberMag_modelMag_min_diff','fiberMag_petroMag_min_diff',\n",
    " 'fiberMag_modelMag_max_diff','petroMag_u_modelMag_u_diff','fiberMag_u_modelMag_u_diff',\n",
    " 'fiberMag_z_petroMag_z_diff','petroMag_modelMag_sum_diff','fiberMag_z_modelMag_z_diff',\n",
    " 'fiberMag_u_petroMag_u_diff','psfMag_z_petroMag_z_diff','petroMag_modelMag_max_diff',\n",
    " 'psfMag_petroMag_max_diff','fiberMag_petroMag_max_diff','fiberMag_petroMag_sum_diff',\n",
    " 'psfMag_u_petroMag_u_diff','petroMag_i_modelMag_z_diff','psfMag_petroMag_sum_diff',\n",
    " 'fiberMag_u_div_mean','fiberMag_z_modelMag_i_diff','petroMag_z_petroMag_i_diff',\n",
    " 'psfMag_z_petroMag_i_diff','psfMag_g_petroMag_r_diff','fiberMag_i_petroMag_z_diff',\n",
    " 'fiberMag_z_petroMag_i_diff','petroMag_z','fiberMag_g_petroMag_u_diff',\n",
    " 'psfMag_i_petroMag_z_diff','petroMag_z_modelMag_i_diff','fiberMag_r_petroMag_i_diff',\n",
    " 'petroMag_sum','fiberMag_r_petroMag_g_diff','psfMag_r_petroMag_i_diff',\n",
    " 'fiberMag_u','psfMag_u','petroMag_max',\n",
    " 'petroMag_r_petroMag_g_diff','psfMag_i_petroMag_r_diff','petroMag_u',\n",
    " 'fiberMag_r_modelMag_z_diff','petroMag_g_modelMag_r_diff','petroMag_diff',\n",
    " 'petroMag_std','fiberMag_z_petroMag_r_diff','psfMag_i_fiberMag_g_diff',\n",
    " 'psfMag_z_petroMag_r_diff','psfMag_g_petroMag_i_diff','fiberMag_r_modelMag_u_diff',\n",
    " 'petroMag_r_modelMag_z_diff','fiberMag_g_petroMag_i_diff','fiberMag_z_modelMag_r_diff',\n",
    " 'psfMag_i_psfMag_g_diff','psfMag_i_petroMag_g_diff','fiberMag_diff',\n",
    " 'petroMag_z_petroMag_r_diff','psfMag_std','modelMag_std',\n",
    " 'modelMag_diff','psfMag_diff','fiberMag_std',\n",
    " 'petroMag_z_modelMag_r_diff','fiberMag_r_fiberMag_u_diff','psfMag_u_petroMag_r_diff',\n",
    " 'fiberMag_u_petroMag_r_diff','psfMag_r_petroMag_z_diff','fiberMag_u_modelMag_r_diff',\n",
    " 'petroMag_i_modelMag_g_diff','petroMag_r_petroMag_u_diff','fiberMag_z_fiberMag_g_diff',\n",
    " 'psfMag_r_petroMag_u_diff','fiberMag_i_petroMag_g_diff','fiberMag_r_petroMag_u_diff',\n",
    " 'psfMag_g_fiberMag_z_diff','petroMag_r_modelMag_u_diff','psfMag_g_modelMag_z_diff',\n",
    " 'petroMag_i_petroMag_g_diff','psfMag_z_fiberMag_g_diff','petroMag_g_modelMag_i_diff',\n",
    " 'fiberMag_z_modelMag_g_diff','modelMag_z_modelMag_g_diff','psfMag_z_psfMag_g_diff',\n",
    " 'fiberMag_g_modelMag_z_diff','fiberMag_z_petroMag_g_diff','psfMag_z_petroMag_g_diff',\n",
    " 'psfMag_g_petroMag_z_diff','petroMag_u_modelMag_r_diff','fiberMag_g_petroMag_z_diff',\n",
    " 'psfMag_i_psfMag_u_diff','psfMag_u_petroMag_i_diff','psfMag_z_petroMag_u_diff',\n",
    " 'petroMag_z_petroMag_g_diff','psfMag_i_fiberMag_u_diff','psfMag_u_fiberMag_i_diff',\n",
    " 'psfMag_u_fiberMag_z_diff','petroMag_z_modelMag_g_diff','psfMag_i_modelMag_u_diff',\n",
    " 'fiberMag_u_modelMag_z_diff','petroMag_g_modelMag_z_diff','fiberMag_u_petroMag_z_diff',\n",
    " 'psfMag_i_petroMag_u_diff','fiberMag_i_fiberMag_u_diff','fiberMag_u_modelMag_i_diff',\n",
    " 'petroMag_i_petroMag_u_diff','psfMag_u_modelMag_z_diff','petroMag_i_modelMag_u_diff',\n",
    " 'psfMag_z_modelMag_u_diff','fiberMag_i_petroMag_u_diff','petroMag_z_petroMag_u_diff',\n",
    " 'psfMag_z_fiberMag_u_diff','petroMag_z_modelMag_u_diff','fiberMag_z_modelMag_u_diff',\n",
    " 'fiberMag_u_petroMag_i_diff','fiberMag_z_petroMag_u_diff','modelMag_z_modelMag_u_diff',\n",
    " 'petroMag_u_modelMag_i_diff','fiberMag_z_fiberMag_u_diff','petroMag_u_modelMag_z_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = [c for c in all_data.columns if c not in ['id','type','type_num']+bad_feature]\n",
    "\n",
    "# diff feature들 간의 표준편차(permutation importance로 걸러낸 것들 중에서..), 실험적으로 찾은 것\n",
    "intersect_good_feature = list(set(diff_feature).intersection(set(train_columns)))\n",
    "all_data['diff_feature_std'] = all_data[intersect_good_feature].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 139 278\n"
     ]
    }
   ],
   "source": [
    "print(len(bad_feature), len(train_columns), len(all_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data.loc[all_data['type'].notnull()]\n",
    "test = all_data.loc[all_data['type'].isnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199991, 256), (10009, 254))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA, FastICA, FactorAnalysis, KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA, LatentDirichletAllocation,MiniBatchSparsePCA, SparsePCA\n",
    "\n",
    "def get_decomposition_feature(train, test, feature, param, decompose_func, prefix):\n",
    "    n_components = param['n_components']\n",
    "    de = decompose_func(**param)\n",
    "    de_train = de.fit_transform(train[feature])\n",
    "    de_test = de.transform(test[feature])\n",
    "    train = pd.concat([train, pd.DataFrame(de_train,columns=[f'{prefix}_{c}' for c in range(n_components)])],axis=1)\n",
    "    test = pd.concat([test, pd.DataFrame(de_test,columns=[f'{prefix}_{c}' for c in range(n_components)])],axis=1)\n",
    "    return train, test\n",
    "\n",
    "org_feature = psfMag_col+fiberMag_col+petroMag_col+modelMag_col\n",
    "# decompostion해서 다시 feature로 추가, 원래 original feature만 사용하고 5개로 축소\n",
    "decom_common_param = {'n_components':5,'random_state':42}\n",
    "train, test = get_decomposition_feature(train, test, org_feature, decom_common_param, TruncatedSVD, 'tsvd5')\n",
    "train, test = get_decomposition_feature(train, test, org_feature, decom_common_param, FastICA, 'ica5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation importance를 사용하여 feature를 제거함\n",
    "# https://eli5.readthedocs.io/en/latest/\n",
    "# lb 0.001~0.003정도 좋아진 것으로 기억\n",
    "print(len(train.columns))\n",
    "train_columns = [c for c in train.columns if c not in ['id','type','type_num']+bad_feature]\n",
    "num_class = train['type'].nunique()\n",
    "\n",
    "print(len(train_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper optimization으로 찾아낸 parameter\n",
    "# lightgbm dart 사용, 보다 lb 0.03 정도 좋음\n",
    "# gbdt가 0.3285라면 dart는 0.3255, goss는 0.3300\n",
    "lgb_param_dart = {'objective': 'multiclass', \n",
    " 'num_class': 19, \n",
    " 'boosting_type': 'dart', \n",
    " 'subsample_freq': 5, \n",
    " 'num_leaves': 92, \n",
    " 'min_data_in_leaf': 64, \n",
    " 'subsample_for_bin': 23000, \n",
    " 'max_depth': 10, \n",
    " 'feature_fraction': 0.302, \n",
    " 'bagging_fraction': 0.904, \n",
    " 'lambda_l1': 0.099, \n",
    " 'lambda_l2': 1.497, \n",
    " 'min_child_weight': 38.011, \n",
    " 'nthread': 32, \n",
    " 'metric': 'multi_logloss', \n",
    " 'learning_rate': 0.021, \n",
    " 'min_sum_hessian_in_leaf': 3, \n",
    " 'drop_rate': 0.846244, \n",
    " 'skip_drop': 0.792465, \n",
    " 'max_drop': 65,\n",
    " 'seed': 42,\n",
    " 'n_estimators': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_columns)\n",
    "print(lgb_param_dart)\n",
    "x_train = train.copy()\n",
    "y_train = train['type_num']\n",
    "x_test = test.copy()\n",
    "\n",
    "num_class = lgb_param_dart['num_class']\n",
    "oof_train = np.zeros((len(x_train),num_class))\n",
    "oof_test = np.zeros((len(x_test),num_class))\n",
    "log_loss_score_list= []   \n",
    "NFOLD = 5\n",
    "SEED = 42\n",
    "\n",
    "# stratifiedkfold 5 fold 사용\n",
    "folds = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=42)\n",
    "for fold_, (trn_index, val_index) in enumerate(folds.split(x_train, y_train)):\n",
    "    print(f\"{fold_+1} FOLD Start!!\")\n",
    "    trn_x, trn_y = x_train.iloc[trn_index][train_columns], y_train.iloc[trn_index]\n",
    "    val_x, val_y = x_train.iloc[val_index][train_columns], y_train.iloc[val_index]\n",
    "    dtrain = lgbm.Dataset(trn_x, label=trn_y, silent=True)\n",
    "    dcross = lgbm.Dataset(val_x, label=val_y, silent=True)\n",
    "    \n",
    "    # dart는 얼리스탑핑이 안되서 한번 num_boost_round를 넉넉히 돌린다음에 5fold에서 가장 좋았던 round로 고정하고 돌린다.\n",
    "    clf = lgbm.train(lgb_param_dart, train_set=dtrain, num_boost_round=1000, valid_sets=[dtrain, dcross], \n",
    "                       verbose_eval=100)\n",
    "    \n",
    "    val_pred = clf.predict(val_x)\n",
    "    oof_train[val_index, :] = val_pred\n",
    "    \n",
    "    log_loss_score = log_loss(val_y, val_pred)\n",
    "    log_loss_score_list.append(log_loss_score)\n",
    "    print(f\"{fold_+1} FOLD LogLoss: \", log_loss_score)\n",
    "    \n",
    "    # 5fold 평균으로 제출\n",
    "    oof_test += clf.predict(x_test[train_columns])/NFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_loss_score_list)\n",
    "np.mean(log_loss_score_list), np.std(log_loss_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission = pd.DataFrame(data=oof_test, columns=sample_submission.columns[1:], index=sample_submission['id'])\n",
    "submission.to_csv('../output/submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
