{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "<br>이제 higher level API(tf.layers 등)를 적극 활용하면서 필요에 따라 앞서 배운 low level API(tf.nn)를 활용해 세부적인 model tuning이 가능합니다. (https://goo.gl/Rmy8qq)\n",
    "<br>\n",
    "<br><span style=\"color:red;\"> - 더욱 편하게 layer 를 구성할 수 있도록 돕는 **tf.layers** 를 적용합니다.\n",
    "<br>- tf.layers.batch_normalization()을 활용해 손쉽게 **Batch Normalization**을 적용할 수 있습니다.</span>\n",
    "<br>- BN을 적용하면 전반적으로 모델의 성능이 향상되어 Params init, Regularization, Dropout 등의 필요성이 크게 줄어듭니다. \n",
    "<br>- 물론 신경망이 깊어지고 풀어야 할 문제가 복잡해진다면 앞선 최적화 방법들을 함께 적용시켜 성능 향상을 도모할 수 있습니다. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 placeholder 들을 선언해줍니다.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "bn_sign = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN 순서 : 선형 결합 -> BN 적용 -> 활성화 함수 \n",
    "# activation function 을 걷어내고 BN을 먼저 적용하기 위해 dense() 대신 fully_connected()를 적용하였습니다.\n",
    "\n",
    "L1 = tf.contrib.layers.fully_connected(X, 256, activation_fn=None)\n",
    "L1 = tf.layers.batch_normalization(L1, training=bn_sign)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "L2 = tf.contrib.layers.fully_connected(L1, 256, activation_fn=None)\n",
    "L2 = tf.layers.batch_normalization(L2, training=bn_sign)\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "model = layers.dense(L2, 10, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.softmax_cross_entropy(Y, model) \n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost = tf.losses.softmax_cross_entropy(Y, model) \n",
    "\n",
    "# BN 적용 중 계산되는 moving_mean & moving_variance 를 지속적으로 업데이트 해줍니다.\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost) \n",
    "    \n",
    "# * When is_training is \"True\", the moving_mean and moving_variance need to be updated, \n",
    "# by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS\n",
    "# so they need to be added as a dependency to the train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "total_batch = mnist.train.num_examples // batch_size\n",
    "total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 || Avg. cost = 0.197 || Training accuracy : 0.933\n",
      "Epoch: 0002 || Avg. cost = 0.077 || Training accuracy : 0.982\n",
      "Epoch: 0003 || Avg. cost = 0.049 || Training accuracy : 0.989\n",
      "Epoch: 0004 || Avg. cost = 0.037 || Training accuracy : 0.993\n",
      "Epoch: 0005 || Avg. cost = 0.028 || Training accuracy : 0.996\n",
      "Epoch: 0006 || Avg. cost = 0.022 || Training accuracy : 0.997\n",
      "Epoch: 0007 || Avg. cost = 0.020 || Training accuracy : 0.997\n",
      "Epoch: 0008 || Avg. cost = 0.018 || Training accuracy : 0.997\n",
      "Epoch: 0009 || Avg. cost = 0.015 || Training accuracy : 0.998\n",
      "Epoch: 0010 || Avg. cost = 0.012 || Training accuracy : 0.998\n",
      "Epoch: 0011 || Avg. cost = 0.013 || Training accuracy : 0.998\n",
      "Epoch: 0012 || Avg. cost = 0.013 || Training accuracy : 0.998\n",
      "Epoch: 0013 || Avg. cost = 0.009 || Training accuracy : 0.999\n",
      "Epoch: 0014 || Avg. cost = 0.010 || Training accuracy : 0.999\n",
      "Epoch: 0015 || Avg. cost = 0.012 || Training accuracy : 0.998\n",
      "Learning process is completed!\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import trange, tqdm_notebook\n",
    "# for epoch in tqdm_notebook(range(15)):\n",
    "\n",
    "for epoch in range(15):\n",
    "    train_cp = [] # Training accuracy 를 동시에 출력해보도록 합니다.\n",
    "    total_cost = 0 # cost\n",
    "\n",
    "    for _ in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, \n",
    "                                                             bn_sign: True}) \n",
    "        \n",
    "        # 매 Epoch마다 Total cost를 출력합니다.\n",
    "        total_cost += cost_val # cost\n",
    "\n",
    "        # 매 Epoch마다 Training accuracy를 출력합니다. (bn_sign을 False로 바꾸어 training mode가 아닌 inference mode로 실행합니다.)\n",
    "        train_cp += sess.run([is_correct], feed_dict={X: batch_xs, Y: batch_ys, \n",
    "                                                      bn_sign: False}) # Training accuracy\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), \n",
    "          '|| Avg. cost =', '{:.3f}'.format(total_cost / total_batch), # cost\n",
    "          '|| Training accuracy : {:.3f}'.format(np.mean(train_cp))) # Training accuracy\n",
    "    \n",
    "print('Learning process is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.9810000061988831\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy 를 출력합니다. \n",
    "# bn_sign을 False로 바꾸어 training mode가 아닌 [ inference mode ]로 바꿔주어야 합니다.\n",
    "# 학습 단계에서는 데이터가 배치 단위로 들어오기 때문에 배치의 평균, 분산을 구하는 것이 가능하지만, \n",
    "# 테스트 단계에서는 배치 단위로 평균/분산을 구하기가 어려워 학습 단계에서 배치 단위의 평균/분산을 저장해 놓고 테스트 시에는 이를 사용합니다.\n",
    "print('Test accuracy : {}'.format(sess.run(accuracy, \n",
    "                                           feed_dict={\n",
    "                                               X: mnist.test.images, \n",
    "                                               Y: mnist.test.labels,\n",
    "                                               bn_sign: False})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
